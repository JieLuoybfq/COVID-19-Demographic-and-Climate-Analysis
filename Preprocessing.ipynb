{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00000-ae194678-1430-451a-bbbb-5f4e3d7e0084",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5656,
    "execution_start": 1610554894555,
    "source_hash": "87326498",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A few packages you may need, uncomment to download via pip !\n",
    "#!pip install reverse_geocoder\n",
    "#!pip install wget\n",
    "#!pip install reverse_geocoder\n",
    "#!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00001-106207f0-ae70-43ad-bffb-86db458082af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 448,
    "execution_start": 1610554733742,
    "output_cleared": false,
    "source_hash": "49700a56"
   },
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import requests\n",
    "import urllib\n",
    "import wget\n",
    "import os\n",
    "import re\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00002-f3c8bc24-ef2c-40e4-b09d-1232cd27b573",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1610554735707,
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "c6dc441b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-4608f95a2a7d>:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = os.path.join(\"data\", \"raw\")\n",
    "clean_path = os.path.join(\"data\", \"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataDir():\n",
    "    \"\"\"\n",
    "    Creates a data directory with clean and raw subdirectories.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "        print(\"created data directory\")\n",
    "        \n",
    "        if not os.path.exists(raw_path):\n",
    "            os.makedirs(raw_path)\n",
    "            print(f\"created {raw_path} directory\")\n",
    "        else:\n",
    "            print(f\"{raw_path} directory exists!\")\n",
    "        \n",
    "        if not os.path.exists(clean_path):\n",
    "            os.makedirs(clean_path)\n",
    "            print(f\"created {clean_path} directory\")     \n",
    "        else:\n",
    "            print(f\"{clean_path} directory exists!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"data directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory exists!\n"
     ]
    }
   ],
   "source": [
    "makeDataDir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-aecc0d6f-912c-4f27-bf56-9c9d3460fa81",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "### 1. Census\n",
    "Two datasets from the ACS 1-year 2019 estimates:\n",
    "\n",
    "-  **DP05** - Demographic and Housing Estimates (exported from the DP05 table at county level summary from [data.census.gov](data.census.gov)).\n",
    "-  **DP03** - Economic Characteristics (exported from the DP03 table at county level summary from [data.census.gov](data.census.gov)).\n",
    "\n",
    "\n",
    "### 2. Climate (temperature and precipitation)\n",
    "The daily Global Historical Climatology Network (GHCN) dataset contains global daily climate data (including temperature and precipitation) across 100,000 stations (read more here: https://www.ncdc.noaa.gov/ghcn-daily-description).\n",
    "\n",
    "\n",
    "### 3. COVID-19 \n",
    "The Johns Hopkins University Center for Systems Science and Engineering COVID-19 dataset contains global daily COVID-19 metrics (including daily case rate) at the County level for US data (read more here: https://github.com/CSSEGISandData/COVID-19).\n",
    "\n",
    "\n",
    "### 4. UID Lookup Table\n",
    "A key for US County names with their respective FIPS codes also found in the Johns Hopkins University Center for Systems Science and Engineering [github repo](https://github.com/CSSEGISandData/COVID-19). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00004-5cced796-c4d5-4858-8adf-0aad36b4c59e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1610554743087,
    "source_hash": "9d32fbe6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def censusApiToDf(year_estimate = \"acs5\", features = False, metadata_only = False, geography = \"county:*&in=state:*\"):\n",
    "    \"\"\"\n",
    "    Takes parameters to extract data for the ACS 2019 DP03 and DP05 datasets.\n",
    "    Returns a dataframe of ACS features or variables at the county level.\n",
    "    \n",
    "    Allows the following parameters to personalize query:\n",
    "    \n",
    "    1. year_estimate - Choose between acs1 and acs5 for 1-year and 5-year estimates.\n",
    "        1-year estimates has more current, but less data.\n",
    "        5-year estimates has less current, but more data.\n",
    "        \n",
    "    2. features - A list of features to extract from ACS data.\n",
    "    \n",
    "    3. metadata_only - If True, returns the metadata of features for a given datatable.\n",
    "    \n",
    "    4. geography - Specify US locations to extract. Defaults to extracting all counties.\n",
    "    \"\"\"\n",
    "    \n",
    "    baseAPI = f\"https://api.census.gov/data/2019/acs/{year_estimate}\"\n",
    "    \n",
    "    \n",
    "    if metadata_only:\n",
    "        url = f\"{baseAPI}/profile/variables\"\n",
    "        \n",
    "    elif features:\n",
    "        features = \",\".join(features)\n",
    "        url = f\"{baseAPI}/profile?get=NAME,{features}&for={geography}\"\n",
    "        \n",
    "    else:\n",
    "        return print(\"Error: Must either set variables = True, or pass input features to extract.\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "    jsonResponse = json.loads(response.text)\n",
    "    \n",
    "    raw = pd.DataFrame(data= jsonResponse)\n",
    "    headers = raw.iloc[0]\n",
    "    raw.columns = headers\n",
    "    df = raw.loc[1:,:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00005-efd38c35-b204-406a-8dcc-9d5db087cba8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 176,
    "execution_start": 1610554747235,
    "source_hash": "71d5970d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "census_metadata = censusApiToDf(metadata_only = True).loc[4:, [\"name\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00006-b1f52913-a4d8-489e-b019-f2f0aa3e0642",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1610554747440,
    "source_hash": "572ffd91",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>DP04_0040E</td>\n",
       "      <td>Estimate!!BEDROOMS!!Total housing units!!1 bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>DP04_0061E</td>\n",
       "      <td>Estimate!!VEHICLES AVAILABLE!!Occupied housing units!!3 or more vehicles available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>DP05_0073PE</td>\n",
       "      <td>Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)!!Puerto Rican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>DP02_0025E</td>\n",
       "      <td>Estimate!!MARITAL STATUS!!Males 15 years and over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>DP02_0110PE</td>\n",
       "      <td>Percent!!WORLD REGION OF BIRTH OF FOREIGN BORN!!Foreign-born population, excluding population born at sea!!Northern America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0           name  \\\n",
       "149  DP04_0040E    \n",
       "544  DP04_0061E    \n",
       "888  DP05_0073PE   \n",
       "909  DP02_0025E    \n",
       "995  DP02_0110PE   \n",
       "\n",
       "0                                                                                                                          label  \n",
       "149  Estimate!!BEDROOMS!!Total housing units!!1 bedroom                                                                           \n",
       "544  Estimate!!VEHICLES AVAILABLE!!Occupied housing units!!3 or more vehicles available                                           \n",
       "888  Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)!!Puerto Rican                       \n",
       "909  Estimate!!MARITAL STATUS!!Males 15 years and over                                                                            \n",
       "995  Percent!!WORLD REGION OF BIRTH OF FOREIGN BORN!!Foreign-born population, excluding population born at sea!!Northern America  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_metadata.sample(frac=1, random_state=8).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-62da186d-7f98-452a-a2a7-03e93d7e3145",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The census metadata files both contain a feature name, and a label description delimited by \"!!\". \n",
    "\n",
    "This can be split in order to better visualize what each feature name refers to and select feature names based on each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00008-81d5ff60-f1fc-45a9-8665-48ffd6564a39",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1610554750491,
    "source_hash": "660d4d1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitCensusMetadata(df):\n",
    "    \"\"\"\n",
    "    Takes the raw census metdata dataframe and splits the name column by the \"!!\" delimiter\n",
    "    \"\"\"\n",
    "    split = df.join(df['label'].str.split(\"!!\", expand=True))\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00009-060ec91a-5e12-471a-b49d-91fdc9d0c491",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 35,
    "execution_start": 1610554751222,
    "source_hash": "aba51587",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>DP04_0040E</td>\n",
       "      <td>Estimate!!BEDROOMS!!Total housing units!!1 bedroom</td>\n",
       "      <td>Estimate</td>\n",
       "      <td>BEDROOMS</td>\n",
       "      <td>Total housing units</td>\n",
       "      <td>1 bedroom</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>DP04_0061E</td>\n",
       "      <td>Estimate!!VEHICLES AVAILABLE!!Occupied housing units!!3 or more vehicles available</td>\n",
       "      <td>Estimate</td>\n",
       "      <td>VEHICLES AVAILABLE</td>\n",
       "      <td>Occupied housing units</td>\n",
       "      <td>3 or more vehicles available</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>DP05_0073PE</td>\n",
       "      <td>Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)!!Puerto Rican</td>\n",
       "      <td>Percent</td>\n",
       "      <td>HISPANIC OR LATINO AND RACE</td>\n",
       "      <td>Total population</td>\n",
       "      <td>Hispanic or Latino (of any race)</td>\n",
       "      <td>Puerto Rican</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>DP02_0025E</td>\n",
       "      <td>Estimate!!MARITAL STATUS!!Males 15 years and over</td>\n",
       "      <td>Estimate</td>\n",
       "      <td>MARITAL STATUS</td>\n",
       "      <td>Males 15 years and over</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>DP02_0110PE</td>\n",
       "      <td>Percent!!WORLD REGION OF BIRTH OF FOREIGN BORN!!Foreign-born population, excluding population born at sea!!Northern America</td>\n",
       "      <td>Percent</td>\n",
       "      <td>WORLD REGION OF BIRTH OF FOREIGN BORN</td>\n",
       "      <td>Foreign-born population, excluding population born at sea</td>\n",
       "      <td>Northern America</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  \\\n",
       "149  DP04_0040E    \n",
       "544  DP04_0061E    \n",
       "888  DP05_0073PE   \n",
       "909  DP02_0025E    \n",
       "995  DP02_0110PE   \n",
       "\n",
       "                                                                                                                           label  \\\n",
       "149  Estimate!!BEDROOMS!!Total housing units!!1 bedroom                                                                            \n",
       "544  Estimate!!VEHICLES AVAILABLE!!Occupied housing units!!3 or more vehicles available                                            \n",
       "888  Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)!!Puerto Rican                        \n",
       "909  Estimate!!MARITAL STATUS!!Males 15 years and over                                                                             \n",
       "995  Percent!!WORLD REGION OF BIRTH OF FOREIGN BORN!!Foreign-born population, excluding population born at sea!!Northern America   \n",
       "\n",
       "            0                                      1  \\\n",
       "149  Estimate  BEDROOMS                                \n",
       "544  Estimate  VEHICLES AVAILABLE                      \n",
       "888  Percent   HISPANIC OR LATINO AND RACE             \n",
       "909  Estimate  MARITAL STATUS                          \n",
       "995  Percent   WORLD REGION OF BIRTH OF FOREIGN BORN   \n",
       "\n",
       "                                                             2  \\\n",
       "149  Total housing units                                         \n",
       "544  Occupied housing units                                      \n",
       "888  Total population                                            \n",
       "909  Males 15 years and over                                     \n",
       "995  Foreign-born population, excluding population born at sea   \n",
       "\n",
       "                                    3             4     5     6  \n",
       "149  1 bedroom                         None          None  None  \n",
       "544  3 or more vehicles available      None          None  None  \n",
       "888  Hispanic or Latino (of any race)  Puerto Rican  None  None  \n",
       "909  None                              None          None  None  \n",
       "995  Northern America                  None          None  None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitCensusMetadata(census_metadata).sample(frac=1, random_state=8).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-a2ce397a-57d8-429d-9605-ceb99746a52d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The first field in the label is the type of measurement being captured, **Percent** or **Estimate**.\n",
    "- For race/ethinicity characteristics of each county, we want **Percent**.\n",
    "- For economic characteristics of each county we want **Percent** insured, and covered by health insurance, as well as the **Estimate** of median household income.\n",
    "\n",
    "Unfortunately, the delimited id fields are of unequal lengths making it difficult to query unique columns of interest across the rows. \n",
    "Instead of filtering by expanded columns, it may be simpler to simply filter on string patterns within the single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00011-3acd9ca5-2901-4045-810d-3f59d36adf9d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1610554752574,
    "source_hash": "37479e26",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findPatterns(df, col, patterns):\n",
    "    \"\"\"\n",
    "    Takes a dataframe, specified columns, and a list of string patterns.\n",
    "    Returns a dataframe with matches to all strings patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    string_masks = (df[col].str.contains(string, regex = True, case = False) for string in patterns)\n",
    "    comb_mask = np.vstack(string_masks).all(axis=0)\n",
    "    final_df = df[comb_mask]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00014-84dcd0fe-935d-4c3b-b91e-c5cd69f1b63c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 52,
    "execution_start": 1610554753885,
    "output_cleared": false,
    "source_hash": "54c659f8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-181e6f94199e>:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  comb_mask = np.vstack(string_masks).all(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>DP05_0052PE</td>\n",
       "      <td>Percent!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>DP05_0044PE</td>\n",
       "      <td>Percent!!RACE!!Total population!!One race!!Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>DP05_0037PE</td>\n",
       "      <td>Percent!!RACE!!Total population!!One race!!White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>DP05_0038PE</td>\n",
       "      <td>Percent!!RACE!!Total population!!One race!!Black or African American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>DP05_0071PE</td>\n",
       "      <td>Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0            name  \\\n",
       "360   DP05_0052PE   \n",
       "493   DP05_0044PE   \n",
       "796   DP05_0037PE   \n",
       "942   DP05_0038PE   \n",
       "1225  DP05_0071PE   \n",
       "\n",
       "0                                                                                        label  \n",
       "360   Percent!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander     \n",
       "493   Percent!!RACE!!Total population!!One race!!Asian                                          \n",
       "796   Percent!!RACE!!Total population!!One race!!White                                          \n",
       "942   Percent!!RACE!!Total population!!One race!!Black or African American                      \n",
       "1225  Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_ethnicity_cols = findPatterns(df = census_metadata,\n",
    "                    col = \"label\",\n",
    "                    patterns= [\"percent!!\", \n",
    "                    \"total\", \n",
    "                    \"one race|hispanic or latino \\(of any race\\)$\", \n",
    "                    \"black|white|pacific islander$|asian$|hispanic\"])\n",
    "\n",
    "filtered_ethnicity_cols = race_ethnicity_cols[~race_ethnicity_cols[\"label\"].str.contains(\"Other Asian|!!Other Pacific Islander\")]\n",
    "DP05_cols = filtered_ethnicity_cols[\"name\"].values\n",
    "filtered_ethnicity_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-56003d0a-68e7-4266-af9c-7beff4d9f898",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Next we need to grab economic metrics including:\n",
    "\n",
    "1. Median income\n",
    "2. Insurance coverage percent\n",
    "3. Poverty percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00015-ac6b032d-8200-4397-a7ae-b2a3e8c8af74",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31,
    "execution_start": 1610554756227,
    "output_cleared": false,
    "source_hash": "3e96a277",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-181e6f94199e>:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  comb_mask = np.vstack(string_masks).all(axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>DP03_0062E</td>\n",
       "      <td>Estimate!!INCOME AND BENEFITS (IN 2019 INFLATION-ADJUSTED DOLLARS)!!Total households!!Median household income (dollars)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>DP03_0096PE</td>\n",
       "      <td>Percent!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!With health insurance coverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>DP03_0119PE</td>\n",
       "      <td>Percent!!PERCENTAGE OF FAMILIES AND PEOPLE WHOSE INCOME IN THE PAST 12 MONTHS IS BELOW THE POVERTY LEVEL!!All families</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0           name  \\\n",
       "159  DP03_0062E    \n",
       "678  DP03_0096PE   \n",
       "675  DP03_0119PE   \n",
       "\n",
       "0                                                                                                                      label  \n",
       "159  Estimate!!INCOME AND BENEFITS (IN 2019 INFLATION-ADJUSTED DOLLARS)!!Total households!!Median household income (dollars)  \n",
       "678  Percent!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!With health insurance coverage             \n",
       "675  Percent!!PERCENTAGE OF FAMILIES AND PEOPLE WHOSE INCOME IN THE PAST 12 MONTHS IS BELOW THE POVERTY LEVEL!!All families   "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_pat = [\"estimate!!\", \"income\", \"median household\"]\n",
    "insurance_pat = [\"percent!!\", \"with health insurance coverage$\", \"noninstitutionalized population!!\"]\n",
    "poverty_pat = [\"percent!!\", \"below the poverty level\", \"all families$\"]\n",
    "\n",
    "economic_cols = pd.concat([findPatterns(census_metadata, \"label\", income_pat),\n",
    "                          findPatterns(census_metadata, \"label\", insurance_pat),\n",
    "                          findPatterns(census_metadata, \"label\", poverty_pat)])\n",
    "\n",
    "DP03_cols = economic_cols[\"name\"].values\n",
    "economic_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-715ac9bd-503e-400b-b186-2ca0dc39e086",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "With our needed features at hand, we can now use the census API to grab county-level data for our features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "00024-43c90db1-cbe8-4c00-8e72-766a25ddacdc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1545,
    "execution_start": 1610554758478,
    "output_cleared": false,
    "source_hash": "c410d3f2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>income_med_dollars</th>\n",
       "      <th>insurance_perc</th>\n",
       "      <th>poverty_perc</th>\n",
       "      <th>pacific_perc</th>\n",
       "      <th>asian_perc</th>\n",
       "      <th>white_perc</th>\n",
       "      <th>black_perc</th>\n",
       "      <th>hispanic_perc</th>\n",
       "      <th>FIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jefferson County, Kentucky</td>\n",
       "      <td>59049</td>\n",
       "      <td>94.1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>22.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>21111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hennepin County, Minnesota</td>\n",
       "      <td>82369</td>\n",
       "      <td>95.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>71.4</td>\n",
       "      <td>13.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Olmsted County, Minnesota</td>\n",
       "      <td>80096</td>\n",
       "      <td>94.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>83.4</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>27109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scott County, Minnesota</td>\n",
       "      <td>108761</td>\n",
       "      <td>95.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>27139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Faulkner County, Arkansas</td>\n",
       "      <td>57642</td>\n",
       "      <td>90.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>82.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>05045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Kaufman County, Texas</td>\n",
       "      <td>75775</td>\n",
       "      <td>86.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>79.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>23.3</td>\n",
       "      <td>48257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Bell County, Texas</td>\n",
       "      <td>54560</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>62.1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>48027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Cameron County, Texas</td>\n",
       "      <td>41123</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>93.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>48061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>El Paso County, Texas</td>\n",
       "      <td>48903</td>\n",
       "      <td>77.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>80.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.9</td>\n",
       "      <td>48141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Nacogdoches County, Texas</td>\n",
       "      <td>47572</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>77.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>48347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0                          NAME income_med_dollars insurance_perc  \\\n",
       "1    Jefferson County, Kentucky  59049              94.1            \n",
       "2    Hennepin County, Minnesota  82369              95.2            \n",
       "3    Olmsted County, Minnesota   80096              94.7            \n",
       "4    Scott County, Minnesota     108761             95.4            \n",
       "5    Faulkner County, Arkansas   57642              90.6            \n",
       "..                         ...     ...               ...            \n",
       "836  Kaufman County, Texas       75775              86.6            \n",
       "837  Bell County, Texas          54560              85.0            \n",
       "838  Cameron County, Texas       41123              70.0            \n",
       "839  El Paso County, Texas       48903              77.8            \n",
       "840  Nacogdoches County, Texas   47572              87.0            \n",
       "\n",
       "0   poverty_perc pacific_perc asian_perc white_perc black_perc hispanic_perc  \\\n",
       "1    9.8          0.1          3.0        71.3       22.4       5.9            \n",
       "2    5.6          0.1          7.3        71.4       13.1       7.0            \n",
       "3    2.7          0.3          6.3        83.4       6.2        5.2            \n",
       "4    2.4          0.0          5.2        83.0       4.9        5.3            \n",
       "5    11.4         0.0          1.1        82.6       12.2       4.2            \n",
       "..    ...         ...          ...         ...        ...       ...            \n",
       "836  8.0          0.0          0.6        79.3       14.1       23.3           \n",
       "837  10.3         0.7          2.9        62.1       25.3       25.6           \n",
       "838  22.6         0.0          0.7        93.7       0.9        90.0           \n",
       "839  16.0         0.0          1.2        80.4       3.5        82.9           \n",
       "840  15.5         0.0          1.1        77.6       19.3       20.0           \n",
       "\n",
       "0     FIPS  \n",
       "1    21111  \n",
       "2    27053  \n",
       "3    27109  \n",
       "4    27139  \n",
       "5    05045  \n",
       "..     ...  \n",
       "836  48257  \n",
       "837  48027  \n",
       "838  48061  \n",
       "839  48141  \n",
       "840  48347  \n",
       "\n",
       "[840 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_features_all = list(DP03_cols) + list(DP05_cols)\n",
    "census_colnames = {\"DP05_0037PE\": \"white_perc\",\n",
    "                    \"DP05_0038PE\": \"black_perc\",\n",
    "                    \"DP05_0039PE\": \"amer_native_perc\",\n",
    "                    \"DP05_0044PE\": \"asian_perc\",\n",
    "                    \"DP05_0052PE\": \"pacific_perc\",\n",
    "                    \"DP05_0071PE\": \"hispanic_perc\",\n",
    "                    \"DP03_0062E\": \"income_med_dollars\",\n",
    "                    \"DP03_0096PE\": \"insurance_perc\",\n",
    "                    \"DP03_0119PE\": \"poverty_perc\"}\n",
    "\n",
    "census_fips = censusApiToDf(year_estimate=\"acs1\", features= census_features_all).rename(columns=census_colnames)\n",
    "census_fips[\"FIPS\"] = census_fips[\"state\"] + census_fips[\"county\"]\n",
    "census_fips.drop(columns = [\"state\", \"county\"], inplace= True)\n",
    "\n",
    "census_fips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-b55a507d-a8dc-4152-8a62-c9e40658a53a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Next, the weather station dataset needs to downloaded, aggregated, and cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "00027-e76846f7-ed5d-4e5b-a8b7-6dc2fb3fc575",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 958,
    "execution_start": 1610554761086,
    "source_hash": "421d5b1e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downloadStations(out = raw_path):\n",
    "    if not os.path.exists(os.path.join(out, \"ghcnd-stations.txt\")):\n",
    "        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "        file = wget.download(url, out = out)\n",
    "    else:\n",
    "        print(\"ghcnd-stations.txt already exists\")\n",
    "\n",
    "def downloadWeather(out = raw_path):\n",
    "    if not os.path.exists(os.path.join(out, \"2020.csv\")):\n",
    "        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz'\n",
    "        wget.download(url, out = out)\n",
    "\n",
    "        !gzip -d data/raw/2020.csv.gz\n",
    "    else:\n",
    "        print(\"2020.csv already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vals(line):\n",
    "    ls = line.split(',')\n",
    "    station = ls[0]\n",
    "    time = ls[1]\n",
    "    val = float(ls[3])\n",
    "    return [station, time, val]\n",
    "\n",
    "def get_stations(filename = os.path.join(raw_path, 'ghcnd-stations.txt')):\n",
    "    df = pd.read_csv(filename, '/t', header=None)\n",
    "    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n",
    "    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n",
    "    df_drop = df[df[\"Station\"].str.startswith(\"US\")].reset_index(drop = True)\n",
    "    return df_drop\n",
    "\n",
    "def process_year(col='TAVG', path = raw_path):\n",
    "    tavg = []\n",
    "    pattern = re.compile(r\"^US.*TAVG\")\n",
    "    with open(os.path.join(path, \"2020.csv\")) as h:\n",
    "        l = h.readline()\n",
    "        while l:\n",
    "            if re.match(pattern, l) is not None:\n",
    "                v = get_vals(l)\n",
    "                tavg.append(get_vals(l))\n",
    "            l = h.readline()\n",
    "    df_tavg = pd.DataFrame(tavg, columns=['Station', 'Date', col])\n",
    "    return df_tavg\n",
    "\n",
    "def mergeStationAggYear(stations, df_tavg):\n",
    "    merged = df_tavg.merge(stations, on='Station', how='inner')\n",
    "    \n",
    "    merged[\"TAVG\"] = merged[\"TAVG\"]/10 + 273\n",
    "\n",
    "    # Get mean temp and temp standard deviation across year\n",
    "    agg_df = merged.loc[:,[\"Station\",\"Latitude\", \"Longitude\", \"TAVG\"]].groupby([\"Station\", \"Latitude\", \"Longitude\"]).agg(\n",
    "        temp_mean = (\"TAVG\", np.mean),\n",
    "        temp_std = (\"TAVG\", np.std))\n",
    "\n",
    "    #Convert temperture mean to celsius for intepretability\n",
    "    agg_df[\"temp_mean\"] = agg_df[\"temp_mean\"] - 273\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weatherToDf():\n",
    "    \"\"\"\n",
    "    The full pipeline to:\n",
    "    1. Download weather station temperature data (US only)\n",
    "    2. Merge station location to temp data\n",
    "    3. Aggregate mean temperature, and standard deviation across the year\n",
    "    \"\"\"\n",
    "    downloadStations()\n",
    "    print(\"Downloaded weather stations\")\n",
    "    downloadWeather()\n",
    "    print(\"Downloaded temperature data.\")\n",
    "    \n",
    "    stations = get_stations()\n",
    "    df_tavg = process_year(col='TAVG')\n",
    "    merged_agg = mergeStationAggYear(stations, df_tavg).reset_index()\n",
    "    \n",
    "    return merged_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "00031-7c959e49-cecc-4ae9-b5df-93a8d7083ac0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1678,
    "execution_start": 1610554766844,
    "source_hash": "77f0bb52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ghcnd-stations.txt already exists\n",
      "Downloaded weather stations\n",
      "2020.csv already exists\n",
      "Downloaded temperature data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-48efbb0076e0>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, '/t', header=None)\n"
     ]
    }
   ],
   "source": [
    "weather_station_agg = weatherToDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>temp_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00244558</td>\n",
       "      <td>48.3042</td>\n",
       "      <td>-114.2636</td>\n",
       "      <td>7.069126</td>\n",
       "      <td>8.606210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00507783</td>\n",
       "      <td>62.0911</td>\n",
       "      <td>-152.7350</td>\n",
       "      <td>-10.887037</td>\n",
       "      <td>5.443052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USR0000AALP</td>\n",
       "      <td>33.8417</td>\n",
       "      <td>-109.1222</td>\n",
       "      <td>10.309563</td>\n",
       "      <td>7.994981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USR0000AASI</td>\n",
       "      <td>67.4750</td>\n",
       "      <td>-162.2664</td>\n",
       "      <td>-3.070083</td>\n",
       "      <td>12.362611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USR0000ABAN</td>\n",
       "      <td>34.1400</td>\n",
       "      <td>-87.3622</td>\n",
       "      <td>16.246721</td>\n",
       "      <td>7.440664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Station Latitude  Longitude  temp_mean   temp_std\n",
       "0  USC00244558  48.3042  -114.2636  7.069126   8.606210 \n",
       "1  USC00507783  62.0911  -152.7350 -10.887037  5.443052 \n",
       "2  USR0000AALP  33.8417  -109.1222  10.309563  7.994981 \n",
       "3  USR0000AASI  67.4750  -162.2664 -3.070083   12.362611\n",
       "4  USR0000ABAN  34.1400  -87.3622   16.246721  7.440664 "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_station_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "00032-fb130a9d-65b6-420a-9a83-0e0bd3159e0b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1610554768530,
    "source_hash": "67570991",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>temp_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2364.000000</td>\n",
       "      <td>2364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.768886</td>\n",
       "      <td>8.659630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.476482</td>\n",
       "      <td>2.107901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.146729</td>\n",
       "      <td>1.235255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.550273</td>\n",
       "      <td>7.618067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.836749</td>\n",
       "      <td>8.554756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.283468</td>\n",
       "      <td>9.383560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26.613934</td>\n",
       "      <td>20.707320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         temp_mean     temp_std\n",
       "count  2364.000000  2364.000000\n",
       "mean   8.768886     8.659630   \n",
       "std    6.476482     2.107901   \n",
       "min   -25.146729    1.235255   \n",
       "25%    4.550273     7.618067   \n",
       "50%    7.836749     8.554756   \n",
       "75%    13.283468    9.383560   \n",
       "max    26.613934    20.707320  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_station_agg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-b6301534-9f46-4f4b-846b-f0e1a6d38fec",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Our aggregated dataset contains 2,364 rows which represents each unique weather station. \n",
    "The weather stations currently have only latitude and longitude. \n",
    "In order to join the weather dataset to the census and covid datasets, the FIPS codes. There are two steps needed to transforms these lat/long values to FIPS codes:\n",
    "\n",
    "1. Convert latitude and longitude to county/state name using reverse geocoder.\n",
    "2. Use a lookup table to extract FIPS codes from county/state name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "00034-7d8579f3-8813-4b36-b49b-b3f1566062b8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1610554771585,
    "source_hash": "939b7ac9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reverse geocode latitutude and longitutes to county and state\n",
    "#Function to reverse_geocode counties\n",
    "\n",
    "def addCountyStateFromLatLong(dataframe, lat, long, US_only = False):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with latitude and longitude and adds the county and state.\n",
    "    \n",
    "    Inputs:\n",
    "    1) Dataframe \n",
    "    2) Latitude Column\n",
    "    3) Longitude Column\n",
    "    \n",
    "    Returns: \n",
    "    1) Dataframe with County and State\n",
    "    \"\"\"\n",
    "    \n",
    "    #Use lat and long as tuples to extract county and state \n",
    "    query = rg.search([tuple(x) for x in dataframe[[lat, long]].values])\n",
    "    \n",
    "    #Initialize lists to extract county and state from our query\n",
    "    state = []\n",
    "    county = []\n",
    "    country = []\n",
    "    \n",
    "    if US_only:\n",
    "        for i in np.arange(0,len(query)):\n",
    "            state.append(query[i][\"admin1\"])\n",
    "            county.append(query[i][\"admin2\"])\n",
    "            country.append(query[i][\"cc\"])\n",
    "    else:\n",
    "        for i in np.arange(0,len(query)):\n",
    "            state.append(query[i][\"admin1\"])\n",
    "            county.append(query[i][\"admin2\"])\n",
    "    \n",
    "    #Create dataframe of filled lists\n",
    "    if US_only:\n",
    "        sc_df = pd.DataFrame({\"state\": state,\n",
    "                        \"county\": county,\n",
    "                        \"country\": country})\n",
    "    else:\n",
    "        sc_df = pd.DataFrame({\"state\": state,\n",
    "                        \"county\": county})\n",
    "    \n",
    "    #assert that the query and station dataframes are the same size\n",
    "    assert len(dataframe) == len(sc_df), \"Row lengths don't match for input and output. Check lat/long values in input.\"\n",
    "    \n",
    "    #Merge the Station with county and state horizontally\n",
    "    if US_only:\n",
    "        concat_df = pd.concat([dataframe,\n",
    "                           sc_df],\n",
    "                           axis = 1)\n",
    "\n",
    "        concat_df = concat_df[concat_df[\"country\"] == \"US\"].drop(columns = \"country\")\n",
    "    else:\n",
    "        concat_df = pd.concat([dataframe,\n",
    "                           sc_df],\n",
    "                           axis = 1)\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "00035-9208ea4c-d336-4abc-a7d5-9800b28321a4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1686,
    "execution_start": 1610554777941,
    "source_hash": "c1b8c227",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "weather_station_state_county = addCountyStateFromLatLong(weather_station_agg, \"Latitude\", \"Longitude\", US_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "00036-5796f82f-8919-4a62-951d-c136ebbeb388",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1610554793667,
    "source_hash": "6e827067",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>temp_std</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00244558</td>\n",
       "      <td>48.3042</td>\n",
       "      <td>-114.2636</td>\n",
       "      <td>7.069126</td>\n",
       "      <td>8.606210</td>\n",
       "      <td>Montana</td>\n",
       "      <td>Flathead County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00507783</td>\n",
       "      <td>62.0911</td>\n",
       "      <td>-152.7350</td>\n",
       "      <td>-10.887037</td>\n",
       "      <td>5.443052</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Kenai Peninsula Borough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USR0000AALP</td>\n",
       "      <td>33.8417</td>\n",
       "      <td>-109.1222</td>\n",
       "      <td>10.309563</td>\n",
       "      <td>7.994981</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Apache County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USR0000AASI</td>\n",
       "      <td>67.4750</td>\n",
       "      <td>-162.2664</td>\n",
       "      <td>-3.070083</td>\n",
       "      <td>12.362611</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Northwest Arctic Borough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USR0000ABAN</td>\n",
       "      <td>34.1400</td>\n",
       "      <td>-87.3622</td>\n",
       "      <td>16.246721</td>\n",
       "      <td>7.440664</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Winston County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Station Latitude  Longitude  temp_mean   temp_std    state  \\\n",
       "0  USC00244558  48.3042  -114.2636  7.069126   8.606210   Montana   \n",
       "1  USC00507783  62.0911  -152.7350 -10.887037  5.443052   Alaska    \n",
       "2  USR0000AALP  33.8417  -109.1222  10.309563  7.994981   Arizona   \n",
       "3  USR0000AASI  67.4750  -162.2664 -3.070083   12.362611  Alaska    \n",
       "4  USR0000ABAN  34.1400  -87.3622   16.246721  7.440664   Alabama   \n",
       "\n",
       "                     county  \n",
       "0  Flathead County           \n",
       "1  Kenai Peninsula Borough   \n",
       "2  Apache County             \n",
       "3  Northwest Arctic Borough  \n",
       "4  Winston County            "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_station_state_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "00037-cb4b314a-1d6e-4ce5-a7b1-16f06184f3d5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1610554795114,
    "source_hash": "3f6b3523",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fipsIntToString(sr):\n",
    "    \"\"\"\n",
    "    Takes a series of FIPS integers with NA values\n",
    "    Returns a FIPS string 5 with 0 padding up front\n",
    "    \"\"\"\n",
    "    string = sr.fillna(0).astype(\"int32\").astype(\"str\")\n",
    "    padding = string.apply(lambda x: \"0\"*(5 - len(x)) + x)\n",
    "    return padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "00038-539f6988-0f5a-4ab0-8a62-c16bc726e159",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1610554795579,
    "source_hash": "3b7ebd44",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper function to join county and state with FIPS from a lookup table\n",
    "def add_FIPS(dataframe):\n",
    "    \"\"\"\n",
    "    This function joins FIPS codes to the state and county outputs of reverse_geocoder\n",
    "    Inputs:\n",
    "    1. Dataframe with \"state\" and \"county\" columns\n",
    "    2. Path to FIPS lookup table\n",
    "    \"\"\"\n",
    "    #Download FIPS lookuptable with relevant columns\n",
    "    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\"\n",
    "    lookuptable= pd.read_csv(url).loc[:,[\"FIPS\", \"Combined_Key\"]]\n",
    "    \n",
    "    #Add the FIPS codes\n",
    "    add_FIPS = dataframe.copy()\n",
    "    add_FIPS[\"Combined_Key\"] = add_FIPS[\"county\"].str.replace(\" County\", \", \") + add_FIPS[\"state\"] + \", US\"\n",
    "    climate_FIPS = add_FIPS.merge(lookuptable, on = \"Combined_Key\")\n",
    "    climate_FIPS.drop(columns = [\"state\", \"county\", \"Combined_Key\"], inplace = True)\n",
    "\n",
    "    #Convert FIPS to 5 digit string\n",
    "    climate_FIPS[\"FIPS\"] = fipsIntToString(climate_FIPS[\"FIPS\"])\n",
    "    return climate_FIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-e6f21210-1dcc-42a2-bebd-1ffb2e1ce020",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Lastly, we want to aggregate all weather stations within each FIPS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "00039-d8f54c3b-1485-48e2-ade4-8776361007b0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 50,
    "execution_start": 1610554798045,
    "source_hash": "63cfabb9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_station_FIPS = add_FIPS(weather_station_state_county)\n",
    "weather_FIPS_agg =  weather_station_FIPS.groupby(\"FIPS\").agg(temp_mean = (\"temp_mean\", np.mean),\n",
    "                                                            temp_std = (\"temp_std\", np.mean)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "00040-47393636-5bc9-40af-8f8e-93283d625a15",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 34,
    "execution_start": 1610554799301,
    "source_hash": "88b9248a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Dataset:\n",
      "Number of Counties: 752\n",
      "Columns = ['FIPS' 'temp_mean' 'temp_std']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weather Dataset:\\nNumber of Counties: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-10022450-3eff-43a2-b388-2e077ccdc98f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "After aggregating temperature features by FIPS codes, we are left with 752 observations.\n",
    "Each of these represent a county with its\n",
    "\n",
    "1. FIPS code\n",
    "2. Temperature mean\n",
    "3. Temperature standard deviation\n",
    "\n",
    "The last dataset that needs to be prepared is the COVID-19 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_id": "00042-ab873485-5d02-49d9-99db-e14dd7e45076",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1610554803351,
    "source_hash": "fa67f8e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downloadCOVID(out = raw_path):\n",
    "    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/12-31-2020.csv\"\n",
    "    \n",
    "    file = url.split(\"/\")[-1]\n",
    "\n",
    "    if os.path.exists(out):\n",
    "        if os.path.exists(os.path.join(out, file)):\n",
    "            print(file, \"already exists\")\n",
    "        else:\n",
    "          wget.download(url=url, out=out)\n",
    "    else:\n",
    "        print(f\"The directory {out} does not exist\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_id": "00043-d9dc1b60-8bc1-4436-aec2-ecb89e1b556e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1610554805845,
    "source_hash": "cbc05d42",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-31-2020.csv already exists\n"
     ]
    }
   ],
   "source": [
    "downloadCOVID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_id": "00044-1fd31fe1-2407-4724-a2d1-89ffca6985bb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1610554806930,
    "output_cleared": false,
    "source_hash": "b3bd7b02"
   },
   "outputs": [],
   "source": [
    "def covidUsDf(path = raw_path, file = \"12-31-2020.csv\"):\n",
    "    \"\"\"\n",
    "    Takes a path and a file containing data from the CSSE COVID-19 Daily Reports\n",
    "\n",
    "    Returns a dataframe with only US data\n",
    "    \"\"\"\n",
    "    raw = pd.read_csv(os.path.join(path, file))\n",
    "    us = raw[raw[\"Country_Region\"] == \"US\"]\n",
    "    return us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00045-90e7e217-a27f-49e6-9ad7-2cc48afb607e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In order to compare each county's COVID-19 infection status adjusted for population, the most useful metric is the **Incident Rate** where:\n",
    "\n",
    "*Incidence Rate = cases per 100,000 persons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_id": "00046-e2505efc-96b0-4fd1-9893-44c2963139eb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 38,
    "execution_start": 1610554822023,
    "source_hash": "934d3b6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Incident_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>01001</td>\n",
       "      <td>7499.686767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>01003</td>\n",
       "      <td>6092.709892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>01005</td>\n",
       "      <td>6133.030868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>01007</td>\n",
       "      <td>8189.693668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>01009</td>\n",
       "      <td>8025.801543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS  Incident_Rate\n",
       "649  01001  7499.686767  \n",
       "650  01003  6092.709892  \n",
       "651  01005  6133.030868  \n",
       "652  01007  8189.693668  \n",
       "653  01009  8025.801543  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_US = covidUsDf().loc[:, [\"FIPS\", \"Incident_Rate\"]]\n",
    "covid_US[\"FIPS\"] = fipsIntToString(covid_US[\"FIPS\"])\n",
    "covid_US.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00047-9693b6e6-bb2f-41d7-9526-a72284741720",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "We now have 3 datasets which can be joined by FIPS codes which represent US counties. The datasets have the following features:\n",
    "\n",
    "1. **Census (2019 estimates)**\n",
    "\n",
    "    - American Native Proportion of the Population\n",
    "    - Asian Proportion of the Population\n",
    "    - Black Proportion of the Population\n",
    "    - Hispanic Proportion of the Population\n",
    "    - White Proportion of the Population\n",
    "    - Income in median dollar (annual per household)\n",
    "    - Proportion of the Population with Insurance\n",
    "    - Proportion of the Population below the Poverty Level\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Climate (2020 aggregate from 1070 weather stations)**\n",
    "\n",
    "    - Temperature mean\n",
    "    - Temperature standard deviation\n",
    "\n",
    "<br> \n",
    "\n",
    "3. **COVID-19 (Cumulative Incidence Rate at the end of 2020, 12-31-2020)**\n",
    "\n",
    "    - Incidence rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_id": "00048-df05cca3-2806-4161-9ab9-fc14f9fd5fbb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 228,
    "execution_start": 1610554825624,
    "source_hash": "fdb31eda",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>income_med_dollars</th>\n",
       "      <th>insurance_perc</th>\n",
       "      <th>poverty_perc</th>\n",
       "      <th>pacific_perc</th>\n",
       "      <th>asian_perc</th>\n",
       "      <th>white_perc</th>\n",
       "      <th>black_perc</th>\n",
       "      <th>hispanic_perc</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>temp_std</th>\n",
       "      <th>Incident_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jefferson County, Kentucky</td>\n",
       "      <td>59049</td>\n",
       "      <td>94.1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>22.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>21111</td>\n",
       "      <td>15.382514</td>\n",
       "      <td>8.917145</td>\n",
       "      <td>6625.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hennepin County, Minnesota</td>\n",
       "      <td>82369</td>\n",
       "      <td>95.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>71.4</td>\n",
       "      <td>13.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27053</td>\n",
       "      <td>8.887978</td>\n",
       "      <td>11.746417</td>\n",
       "      <td>6815.458157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olmsted County, Minnesota</td>\n",
       "      <td>80096</td>\n",
       "      <td>94.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>83.4</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>27109</td>\n",
       "      <td>7.541530</td>\n",
       "      <td>11.488897</td>\n",
       "      <td>5880.866494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sedgwick County, Kansas</td>\n",
       "      <td>59716</td>\n",
       "      <td>89.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>77.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20173</td>\n",
       "      <td>14.396175</td>\n",
       "      <td>9.938393</td>\n",
       "      <td>7639.494460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allegheny County, Pennsylvania</td>\n",
       "      <td>64871</td>\n",
       "      <td>96.3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>79.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>42003</td>\n",
       "      <td>11.866667</td>\n",
       "      <td>9.198555</td>\n",
       "      <td>4424.918486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Wichita County, Texas</td>\n",
       "      <td>51749</td>\n",
       "      <td>84.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>81.3</td>\n",
       "      <td>9.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>48485</td>\n",
       "      <td>17.574863</td>\n",
       "      <td>8.689958</td>\n",
       "      <td>8144.899040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Gregg County, Texas</td>\n",
       "      <td>53793</td>\n",
       "      <td>80.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>74.1</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.2</td>\n",
       "      <td>48183</td>\n",
       "      <td>18.798634</td>\n",
       "      <td>7.568277</td>\n",
       "      <td>5495.986123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Lubbock County, Texas</td>\n",
       "      <td>55003</td>\n",
       "      <td>86.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>80.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>36.3</td>\n",
       "      <td>48303</td>\n",
       "      <td>15.357377</td>\n",
       "      <td>9.507474</td>\n",
       "      <td>13484.282076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Cameron County, Texas</td>\n",
       "      <td>41123</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>93.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>48061</td>\n",
       "      <td>24.348224</td>\n",
       "      <td>5.120266</td>\n",
       "      <td>6977.216817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>El Paso County, Texas</td>\n",
       "      <td>48903</td>\n",
       "      <td>77.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>80.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.9</td>\n",
       "      <td>48141</td>\n",
       "      <td>20.448361</td>\n",
       "      <td>9.119162</td>\n",
       "      <td>11741.603693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               NAME income_med_dollars insurance_perc  \\\n",
       "0    Jefferson County, Kentucky      59049              94.1            \n",
       "1    Hennepin County, Minnesota      82369              95.2            \n",
       "2    Olmsted County, Minnesota       80096              94.7            \n",
       "3    Sedgwick County, Kansas         59716              89.3            \n",
       "4    Allegheny County, Pennsylvania  64871              96.3            \n",
       "..                              ...    ...               ...            \n",
       "298  Wichita County, Texas           51749              84.9            \n",
       "299  Gregg County, Texas             53793              80.2            \n",
       "300  Lubbock County, Texas           55003              86.0            \n",
       "301  Cameron County, Texas           41123              70.0            \n",
       "302  El Paso County, Texas           48903              77.8            \n",
       "\n",
       "    poverty_perc pacific_perc asian_perc white_perc black_perc hispanic_perc  \\\n",
       "0    9.8          0.1          3.0        71.3       22.4       5.9            \n",
       "1    5.6          0.1          7.3        71.4       13.1       7.0            \n",
       "2    2.7          0.3          6.3        83.4       6.2        5.2            \n",
       "3    7.7          0.0          4.3        77.8       8.6        15.0           \n",
       "4    6.6          0.0          3.9        79.4       12.9       2.3            \n",
       "..   ...          ...          ...         ...        ...       ...            \n",
       "298  7.2          0.2          2.3        81.3       9.6        19.8           \n",
       "299  13.4         0.0          1.3        74.1       18.4       19.2           \n",
       "300  13.4         0.2          2.2        80.9       6.9        36.3           \n",
       "301  22.6         0.0          0.7        93.7       0.9        90.0           \n",
       "302  16.0         0.0          1.2        80.4       3.5        82.9           \n",
       "\n",
       "      FIPS  temp_mean   temp_std  Incident_Rate  \n",
       "0    21111  15.382514  8.917145   6625.828000    \n",
       "1    27053  8.887978   11.746417  6815.458157    \n",
       "2    27109  7.541530   11.488897  5880.866494    \n",
       "3    20173  14.396175  9.938393   7639.494460    \n",
       "4    42003  11.866667  9.198555   4424.918486    \n",
       "..     ...        ...       ...           ...    \n",
       "298  48485  17.574863  8.689958   8144.899040    \n",
       "299  48183  18.798634  7.568277   5495.986123    \n",
       "300  48303  15.357377  9.507474   13484.282076   \n",
       "301  48061  24.348224  5.120266   6977.216817    \n",
       "302  48141  20.448361  9.119162   11741.603693   \n",
       "\n",
       "[303 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_weather_covid_merge = census_fips.merge(weather_FIPS_agg, how= \"inner\", on = \"FIPS\").merge(covid_US, how= \"inner\", on = \"FIPS\")\n",
    "census_weather_covid_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00049-8ed73d8b-d990-40fe-899b-f3239d995621",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "According to the United States Census Bureau, there are 3,141 county-equivalents in the U.S., but our merged dataset only contains 228 records. \n",
    "Which datasets are lacking the most counties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_id": "00050-275edf67-3966-44d0-ab2f-94bec25d2ea8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1610554828794,
    "source_hash": "f1d3a527",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fipsGetAll():\n",
    "    \"\"\"\n",
    "    Downloads and returns all unique US FIPs codes from kjhealy's github repo as integers.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/kjhealy/fips-codes/master/county_fips_master.csv\", \n",
    "                     encoding = 'cp1252')\n",
    "    fips = df.loc[:,[\"fips\"]].rename(columns = {\"fips\": \"FIPS\"})\n",
    "    return fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_id": "00051-04317b63-b750-4154-a2f4-9f0b7ea83bb3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 169,
    "execution_start": 1610554829690,
    "source_hash": "9ec4c540",
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_fips = fipsGetAll()\n",
    "all_fips[\"FIPS\"] = fipsIntToString(all_fips[\"FIPS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_id": "00052-2d25dee0-f26c-48dd-9ec1-0cf436351d5e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1610554835929,
    "source_hash": "f51fdb7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fipsCoverage(df, allfips):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with some FIPS codes and compares it to another dataframe with all FIPS codes, both of which must have a \"FIPS\" columns.\n",
    "    Returns the percent of FIPS codes covered in the first dataframe.\n",
    "    \"\"\"\n",
    "    proportion = int(df.merge(allfips, on = \"FIPS\").shape[0]/ allfips.shape[0]*100)\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_id": "00053-136a63c8-edfe-4b70-81fa-ab02c8c9d26e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1610554837342,
    "source_hash": "f516d73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 840 counties in the Census dataset with 26% coverage.\n",
      "There are 752 counties in the Climate dataset with 23% coverage.\n",
      "There are 3274 counties in the COVID-19 dataset with 99% coverage.\n",
      "There are 303 counties in the merged dataset with 9% coverage.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {census_fips.shape[0]} counties in the Census dataset with {fipsCoverage(census_fips, all_fips)}% coverage.\")\n",
    "print(f\"There are {weather_FIPS_agg.shape[0]} counties in the Climate dataset with {fipsCoverage(weather_FIPS_agg, all_fips)}% coverage.\")\n",
    "print(f\"There are {covid_US.shape[0]} counties in the COVID-19 dataset with {fipsCoverage(covid_US, all_fips)}% coverage.\")\n",
    "print(f\"There are {census_weather_covid_merge.shape[0]} counties in the merged dataset with {fipsCoverage(census_weather_covid_merge, all_fips)}% coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-a257eef8-db4e-4f78-b357-174b2fe32438",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In order to geographically visualize which US counties are covered in our datasets, we can map the FIPS codes present in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_id": "00055-7f59034a-8565-4906-98ea-a4e60ae8e294",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1610555269585,
    "source_hash": "1a02c85c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map of U.S. Counties\n",
    "def mapCounties(df, title = \"US Counties\", savefig = False, fig_dir =\"Figures\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a dataframe with a \"FIPS\" column.\n",
    "    Returns a map of the US with the present counties highlighted.\n",
    "    \"\"\"    \n",
    "\n",
    "    with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "        counties = json.load(response)\n",
    "\n",
    "    fig = px.choropleth(df, \n",
    "                        geojson=counties, \n",
    "                        locations='FIPS',\n",
    "                        scope=\"usa\")\n",
    "\n",
    "    fig.update_layout(title = title,\n",
    "                      showlegend = False)\n",
    "    if savefig:\n",
    "        if \"html\" in savefig:\n",
    "            return plotly.offline.plot(fig, filename= os.path.join(fig_dir, savefig))\n",
    "        else:\n",
    "            return fig.write_image(os.path.join(fig_dir, savefig))\n",
    "    else:\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_id": "00056-7b5817af-20bc-4910-81fb-938317be96a0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5789,
    "execution_start": 1610555270422,
    "source_hash": "b1379bd3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapCounties(census_fips, \n",
    "title = f'Counties in United States 2019 Census Estimates ({fipsCoverage(census_fips, all_fips)}% coverage)',\n",
    "savefig= \"Counties_2019_ACS1_Census.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_id": "00058-585f093f-35ae-4915-930f-49bdd311ee6c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5563,
    "execution_start": 1610555366589,
    "source_hash": "f55e0102",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapCounties(weather_FIPS_agg, \n",
    "title= f\"Counties in GHCN weather dataset ({fipsCoverage(weather_FIPS_agg, all_fips)}% coverage)\",\n",
    "savefig= \"Counties_2019_Weather.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_id": "00059-f8a57ada-144a-4e29-91bc-be2dea08d1a1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6192,
    "execution_start": 1610555401261,
    "source_hash": "81de1f34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapCounties(covid_US, \n",
    "title= f\"Counties in COVID-19 dataset ({fipsCoverage(covid_US, all_fips)}% coverage)\",\n",
    "savefig= \"Counties_2019_COVID19.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_id": "00060-c413f9dd-9f71-4197-9312-f8aaa802ebbd",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5287,
    "execution_start": 1610555536041,
    "source_hash": "a9ff0e59",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapCounties(census_weather_covid_merge, \n",
    "title= f\"Counties in Merged Census, Weather, and COVID Dataset({fipsCoverage(census_weather_covid_merge, all_fips)}% coverage)\", \n",
    "savefig= \"Counties_2019_ACS1_Census_Weather_COVID.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-d4e180f3-c09b-4bd2-86d7-e04ab7a9b2bf",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From the maps we can see that:\n",
    "\n",
    "1. The **COVID-19** dataset has the most coverage, with data on nearly all counties.\n",
    "2. The **Census 2019 estimates** dataset has moderate coverage, with sparse coverage in the Western US.\n",
    "3. The **Weather** dataset has modertely low coverage, with sparse coverage in the Eastern US.\n",
    "\n",
    "After performing inner joins on all three datasets, the full dataset is very limited.\n",
    "\n",
    "The final, merged, census, weather, and covid dataset has only has 303 records which represents only 9% of all US counties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00062-400343c0-91c4-431c-ad8f-06abbb3dd26e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There are several reasons why we have such little coverage of US counties including:\n",
    "\n",
    "1. **Weather stations are not distrubted evenly among counties**. \n",
    "    - They appear to be most prevalent along the Rocky Mountains in the Western US.\n",
    "    - They are sparsley distributed in the MidWest and Eastern US.\n",
    "2. **The census data is missing many counties**\n",
    "    - The 1-year 2019 ACS census estimates miss many counties in order to present only the most current information:\n",
    "        - Only including 2019 data\n",
    "        - Only including counties with populations > 65,000\n",
    "        - To read more about the tradeoff between 1-year, 3-year, and 5-year estimates, visit https://www.census.gov/programs-surveys/acs/guidance/estimates.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00063-8ee1d6ec-36b8-4bd4-9644-fc73715aa0d7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "This **smaller dataset with the most current feature estimates seems the most appropriate for inference** regarding the effect of demographic and weather features on COVID-19 infection rates.\n",
    "\n",
    "**For predictions, we can instead want to use the larger, but less current 5-year ACS census estimates**.\n",
    "Since the Climate dataset 13% coverage, we will keep weather out of the predictive modeling features leaving only demographic features to predict COVID-19 infection rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cell_id": "00056-187f25bc-540d-4355-9746-ed066efc83ab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1157,
    "execution_start": 1610557089457,
    "source_hash": "70ec420e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the same features extracted for the acs1 census estimates\n",
    "\n",
    "acs5_census= censusApiToDf(year_estimate=\"acs5\", features= census_features_all).rename(columns=census_colnames)\n",
    "acs5_census[\"FIPS\"] = acs5_census[\"state\"] + acs5_census[\"county\"]\n",
    "acs5_census.drop(columns = [\"state\", \"county\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cell_id": "00057-e689d397-152d-4041-b533-a14d48dc62fe",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1610559165673,
    "source_hash": "ac871da5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "acs5_covid_merge = acs5_census.merge(covid_US, how= \"inner\", on = \"FIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_id": "00058-543b8790-4d5b-4c37-9208-9305f91f4af6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 66,
    "execution_start": 1610559174838,
    "source_hash": "fccb2d57",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>income_med_dollars</th>\n",
       "      <th>insurance_perc</th>\n",
       "      <th>poverty_perc</th>\n",
       "      <th>pacific_perc</th>\n",
       "      <th>asian_perc</th>\n",
       "      <th>white_perc</th>\n",
       "      <th>black_perc</th>\n",
       "      <th>hispanic_perc</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Incident_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fayette County, Illinois</td>\n",
       "      <td>46650</td>\n",
       "      <td>91.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>93.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>17051</td>\n",
       "      <td>12485.939258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logan County, Illinois</td>\n",
       "      <td>57308</td>\n",
       "      <td>95.5</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>88.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>17107</td>\n",
       "      <td>9452.093088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saline County, Illinois</td>\n",
       "      <td>44090</td>\n",
       "      <td>95.8</td>\n",
       "      <td>17.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>92.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>17165</td>\n",
       "      <td>7509.258865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lake County, Illinois</td>\n",
       "      <td>89427</td>\n",
       "      <td>93.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>75.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>17097</td>\n",
       "      <td>6934.324908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Massac County, Illinois</td>\n",
       "      <td>47481</td>\n",
       "      <td>94.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>17127</td>\n",
       "      <td>6484.170781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>Crockett County, Tennessee</td>\n",
       "      <td>44717</td>\n",
       "      <td>88.9</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>79.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>47033</td>\n",
       "      <td>11531.974701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>Lake County, Tennessee</td>\n",
       "      <td>35191</td>\n",
       "      <td>88.7</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>67.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>47095</td>\n",
       "      <td>20068.415051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>Knox County, Tennessee</td>\n",
       "      <td>57470</td>\n",
       "      <td>92.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>85.5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>47093</td>\n",
       "      <td>6942.610559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>Benton County, Washington</td>\n",
       "      <td>69023</td>\n",
       "      <td>92.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>82.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>21.7</td>\n",
       "      <td>53005</td>\n",
       "      <td>5720.436421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>Clark County, Washington</td>\n",
       "      <td>75253</td>\n",
       "      <td>94.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>84.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>53011</td>\n",
       "      <td>2733.076493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3194 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            NAME income_med_dollars insurance_perc  \\\n",
       "0     Fayette County, Illinois    46650              91.8            \n",
       "1     Logan County, Illinois      57308              95.5            \n",
       "2     Saline County, Illinois     44090              95.8            \n",
       "3     Lake County, Illinois       89427              93.2            \n",
       "4     Massac County, Illinois     47481              94.6            \n",
       "...                       ...       ...               ...            \n",
       "3189  Crockett County, Tennessee  44717              88.9            \n",
       "3190  Lake County, Tennessee      35191              88.7            \n",
       "3191  Knox County, Tennessee      57470              92.2            \n",
       "3192  Benton County, Washington   69023              92.7            \n",
       "3193  Clark County, Washington    75253              94.2            \n",
       "\n",
       "     poverty_perc pacific_perc asian_perc white_perc black_perc hispanic_perc  \\\n",
       "0     12.8         0.1          0.5        93.9       4.7        1.9            \n",
       "1     5.6          0.0          0.8        88.5       6.9        3.4            \n",
       "2     17.9         0.2          0.7        92.7       2.6        1.8            \n",
       "3     5.8          0.0          7.7        75.8       6.8        21.7           \n",
       "4     13.6         0.0          0.2        91.1       5.8        2.9            \n",
       "...    ...         ...          ...         ...       ...        ...            \n",
       "3189  13.1         0.0          0.3        79.5       13.0       10.6           \n",
       "3190  25.6         0.0          0.2        67.5       28.5       2.3            \n",
       "3191  9.5          0.0          2.2        85.5       8.7        4.3            \n",
       "3192  8.6          0.1          2.6        82.1       1.6        21.7           \n",
       "3193  5.8          0.8          4.6        84.6       1.8        9.6            \n",
       "\n",
       "       FIPS  Incident_Rate  \n",
       "0     17051  12485.939258   \n",
       "1     17107  9452.093088    \n",
       "2     17165  7509.258865    \n",
       "3     17097  6934.324908    \n",
       "4     17127  6484.170781    \n",
       "...     ...          ...    \n",
       "3189  47033  11531.974701   \n",
       "3190  47095  20068.415051   \n",
       "3191  47093  6942.610559    \n",
       "3192  53005  5720.436421    \n",
       "3193  53011  2733.076493    \n",
       "\n",
       "[3194 rows x 11 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs5_covid_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_id": "00057-4a7b5185-09b2-4aa1-a3d2-194e69ef876f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1610559234061,
    "source_hash": "eae95fdb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3220 counties in the ACS 5-year Census dataset with 99% coverage.\n",
      "There are 3194 counties in the merged prediction dataset with 99% coverage.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {acs5_census.shape[0]} counties in the ACS 5-year Census dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")\n",
    "print(f\"There are {acs5_covid_merge.shape[0]} counties in the merged prediction dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapCounties(acs5_census, \n",
    "title= f\"Counties in acs5 dataset ({fipsCoverage(acs5_census, all_fips)}% coverage)\",\n",
    "savefig= \"Counties_2019_ACS5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00058-263adea0-d848-436a-9220-19d595542c51",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The 5-year estimates contain an incredible 99% coverage! The 3,220 records has sufficient data for training and validation in our predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_id": "00064-fa9dd6c9-a909-4153-8403-0b054ae8e66e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1610559288299,
    "source_hash": "1b95f92b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_csv(df, path, filename):\n",
    "    \"\"\"\n",
    "    Takes a dataframe, path, and filename.\n",
    "    Returns a written csv.\n",
    "    Checks if the csv is already written, and if the path is correct.\n",
    "    \"\"\"\n",
    "    file = os.path.join(path, filename)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        if os.path.exists(file):\n",
    "            print(f\"The csv {filename} already exists.\")\n",
    "        else:\n",
    "            pd.DataFrame.to_csv(df, file)\n",
    "    else:\n",
    "        print(f\"The path {path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_id": "00065-e62e108b-74ee-48af-a158-193f9cd4efe8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1610559289448,
    "source_hash": "e9b641f7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv census_weather_covid_inference.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "census_weather_covid_inference_name = \"census_weather_covid_inference.csv\"\n",
    "\n",
    "write_csv(census_weather_covid_merge, clean_path, census_weather_covid_inference_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cell_id": "00058-d8e5fb0d-e3d9-4c8b-baec-237f2a4778e0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1610559292321,
    "source_hash": "1f20bedc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv census_covid_prediction.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "census_covid_prediction_name = \"census_covid_prediction.csv\"\n",
    "write_csv(acs5_covid_merge, clean_path, census_covid_prediction_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00064-d8e4f74c-1bf4-4122-9de6-dae56e2dba26",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With our 2 preprocessed datasets, we can now move on to analysis."
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "bcedd8ad-6356-4b65-93bf-25545ea15904",
  "history": [
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport requests\nimport wget\nimport os\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "idx": 1,
    "time": "2021-01-25T19:41:07.353Z",
    "type": "execution"
   },
   {
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "time": "2021-01-25T19:41:09.149Z",
    "type": "completion"
   },
   {
    "code": "!pip install reverse_geocoder\n#!pip install wget\n#!pip install reverse_geocoder\n#!pip install kaleido",
    "id": "01e2ea92db864e4b8ff0f1569f226be0",
    "idx": 0,
    "time": "2021-01-25T19:41:22.121Z",
    "type": "execution"
   },
   {
    "id": "01e2ea92db864e4b8ff0f1569f226be0",
    "time": "2021-01-25T19:41:26.659Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport requests\nimport wget\nimport os\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "idx": 1,
    "time": "2021-01-25T19:41:46.991Z",
    "type": "execution"
   },
   {
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "time": "2021-01-25T19:41:47.227Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "idx": 1,
    "time": "2021-01-25T19:42:04.714Z",
    "type": "execution"
   },
   {
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "time": "2021-01-25T19:42:04.787Z",
    "type": "completion"
   },
   {
    "code": "pd.set_option('display.max_rows', 200)\npd.set_option('display.max_colwidth', -1)",
    "id": "da9de52af5124c9c8f2e52bf17931aad",
    "idx": 2,
    "time": "2021-01-25T19:42:15.377Z",
    "type": "execution"
   },
   {
    "id": "da9de52af5124c9c8f2e52bf17931aad",
    "time": "2021-01-25T19:42:15.514Z",
    "type": "completion"
   },
   {
    "code": "def censusApiToDf(year_estimate = \"acs5\", features = False, metadata_only = False, geography = \"county:*&in=state:*\"):\n    \"\"\"\n    Takes parameters to extract data for the ACS 2019 DP03 and DP05 datasets.\n    Returns a dataframe of ACS features or variables at the county level.\n    \n    Allows the following parameters to personalize query:\n    \n    1. year_estimate - Choose between acs1 and acs5 for 1-year and 5-year estimates.\n        1-year estimates has more current, but less data.\n        5-year estimates has less current, but more data.\n        \n    2. features - A list of features to extract from ACS data.\n    \n    3. metadata_only - If True, returns the metadata of features for a given datatable.\n    \n    4. geography - Specify US locations to extract. Defaults to extracting all counties.\n    \"\"\"\n    \n    baseAPI = f\"https://api.census.gov/data/2019/acs/{year_estimate}\"\n    \n    \n    if metadata_only:\n        url = f\"{baseAPI}/profile/variables\"\n        \n    elif features:\n        features = \",\".join(features)\n        url = f\"{baseAPI}/profile?get=NAME,{features}&for={geography}\"\n        \n    else:\n        return print(\"Error: Must either set variables = True, or pass input features to extract.\")\n\n    response = requests.get(url)\n    jsonResponse = json.loads(response.text)\n    \n    raw = pd.DataFrame(data= jsonResponse)\n    headers = raw.iloc[0]\n    raw.columns = headers\n    df = raw.loc[1:,:]\n    \n    return df",
    "id": "49411a6f82c448378927a39c205d3c7c",
    "idx": 4,
    "time": "2021-01-25T19:42:50.250Z",
    "type": "execution"
   },
   {
    "id": "49411a6f82c448378927a39c205d3c7c",
    "time": "2021-01-25T19:42:50.312Z",
    "type": "completion"
   },
   {
    "code": "census_metadata = censusApiToDf(metadata_only = True).loc[4:, [\"name\", \"label\"]]",
    "id": "26c7f2fb208c419b815fe2196579aeda",
    "idx": 5,
    "time": "2021-01-25T19:43:01.547Z",
    "type": "execution"
   },
   {
    "id": "26c7f2fb208c419b815fe2196579aeda",
    "time": "2021-01-25T19:43:01.924Z",
    "type": "completion"
   },
   {
    "code": "census_metadata.sample(frac=1, random_state=8).head()",
    "id": "dff7e1e754a84f379ba523566b781ea4",
    "idx": 6,
    "time": "2021-01-25T19:43:02.951Z",
    "type": "execution"
   },
   {
    "id": "dff7e1e754a84f379ba523566b781ea4",
    "time": "2021-01-25T19:43:03.060Z",
    "type": "completion"
   },
   {
    "code": "def splitCensusMetadata(df):\n    \"\"\"\n    Takes the raw census metdata dataframe and splits the name column by the \"!!\" delimiter\n    \"\"\"\n    split = df.join(df['label'].str.split(\"!!\", expand=True))\n    return split",
    "id": "d01ed0351ece4940840f15ca09538f3c",
    "idx": 8,
    "time": "2021-01-25T19:43:10.049Z",
    "type": "execution"
   },
   {
    "id": "d01ed0351ece4940840f15ca09538f3c",
    "time": "2021-01-25T19:43:10.105Z",
    "type": "completion"
   },
   {
    "code": "splitCensusMetadata(census_metadata).sample(frac=1, random_state=8).head()",
    "id": "8321f47caa744f91a1f185fb0eb65207",
    "idx": 9,
    "time": "2021-01-25T19:43:11.245Z",
    "type": "execution"
   },
   {
    "id": "8321f47caa744f91a1f185fb0eb65207",
    "time": "2021-01-25T19:43:11.354Z",
    "type": "completion"
   },
   {
    "code": "def findPatterns(df, col, patterns):\n    \"\"\"\n    Takes a dataframe, specified columns, and a list of string patterns.\n    Returns a dataframe with matches to all strings patterns.\n    \"\"\"\n\n    string_masks = (df[col].str.contains(string, regex = True, case = False) for string in patterns)\n    comb_mask = np.vstack(string_masks).all(axis=0)\n    final_df = df[comb_mask]\n\n    return final_df",
    "id": "2277efa83c154460b3f381ca00e2cf59",
    "idx": 11,
    "time": "2021-01-25T19:43:17.835Z",
    "type": "execution"
   },
   {
    "id": "2277efa83c154460b3f381ca00e2cf59",
    "time": "2021-01-25T19:43:17.898Z",
    "type": "completion"
   },
   {
    "code": "race_ethnicity_cols = findPatterns(df = census_metadata,\n                    col = \"label\",\n                    patterns= [\"percent!!\", \n                    \"total\", \n                    \"one race|hispanic or latino \\(of any race\\)$\", \n                    \"black|white|pacific islander$|asian$|hispanic\"])\n\nfiltered_ethnicity_cols = race_ethnicity_cols[~race_ethnicity_cols[\"label\"].str.contains(\"Other Asian|!!Other Pacific Islander\")]\nDP05_cols = filtered_ethnicity_cols[\"name\"].values\nfiltered_ethnicity_cols",
    "id": "761ac31f40e64e5f8910b7a8faf607be",
    "idx": 12,
    "time": "2021-01-25T19:43:20.175Z",
    "type": "execution"
   },
   {
    "id": "761ac31f40e64e5f8910b7a8faf607be",
    "time": "2021-01-25T19:43:20.290Z",
    "type": "completion"
   },
   {
    "code": "income_pat = [\"estimate!!\", \"income\", \"median household\"]\ninsurance_pat = [\"percent!!\", \"with health insurance coverage$\", \"noninstitutionalized population!!\"]\npoverty_pat = [\"percent!!\", \"below the poverty level\", \"all families$\"]\n\neconomic_cols = pd.concat([findPatterns(census_metadata, \"label\", income_pat),\n                          findPatterns(census_metadata, \"label\", insurance_pat),\n                          findPatterns(census_metadata, \"label\", poverty_pat)])\n\nDP03_cols = economic_cols[\"name\"].values\neconomic_cols",
    "id": "0ee15f68752d4be9bfecb0ced328746b",
    "idx": 14,
    "time": "2021-01-25T19:43:25.800Z",
    "type": "execution"
   },
   {
    "id": "0ee15f68752d4be9bfecb0ced328746b",
    "time": "2021-01-25T19:43:25.917Z",
    "type": "completion"
   },
   {
    "code": "census_features_all = list(DP03_cols) + list(DP05_cols)\ncensus_colnames = {\"DP05_0037PE\": \"white_perc\",\n                    \"DP05_0038PE\": \"black_perc\",\n                    \"DP05_0039PE\": \"amer_native_perc\",\n                    \"DP05_0044PE\": \"asian_perc\",\n                    \"DP05_0052PE\": \"pacific_perc\",\n                    \"DP05_0071PE\": \"hispanic_perc\",\n                    \"DP03_0062E\": \"income_med_dollars\",\n                    \"DP03_0096PE\": \"insurance_perc\",\n                    \"DP03_0119PE\": \"poverty_perc\"}\n\ncensus_fips = censusApiToDf(year_estimate=\"acs1\", features= census_features_all).rename(columns=census_colnames)\ncensus_fips[\"FIPS\"] = census_fips[\"state\"] + census_fips[\"county\"]\ncensus_fips.drop(columns = [\"state\", \"county\"], inplace= True)\n\ncensus_fips",
    "id": "5fae843092b9463d8c0781b24c65ddcc",
    "idx": 16,
    "time": "2021-01-25T19:43:38.256Z",
    "type": "execution"
   },
   {
    "id": "5fae843092b9463d8c0781b24c65ddcc",
    "time": "2021-01-25T19:43:39.665Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "idx": 3,
    "time": "2021-01-25T19:45:29.057Z",
    "type": "execution"
   },
   {
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "time": "2021-01-25T19:45:29.218Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "idx": 4,
    "time": "2021-01-25T19:45:30.879Z",
    "type": "execution"
   },
   {
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "time": "2021-01-25T19:45:31.095Z",
    "type": "completion"
   },
   {
    "code": "raw_path = os.path.join(\"data\", \"raw\")\nclean_path = os.path.join(\"data\", \"clean\")",
    "id": "b9256790c854440e8efb30c93972219a",
    "idx": 3,
    "time": "2021-01-25T19:45:45.482Z",
    "type": "execution"
   },
   {
    "id": "b9256790c854440e8efb30c93972219a",
    "time": "2021-01-25T19:45:45.539Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "idx": 4,
    "time": "2021-01-25T19:45:45.898Z",
    "type": "execution"
   },
   {
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "time": "2021-01-25T19:45:45.954Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "idx": 5,
    "time": "2021-01-25T19:45:46.878Z",
    "type": "execution"
   },
   {
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "time": "2021-01-25T19:45:46.975Z",
    "type": "completion"
   },
   {
    "code": "raw_path = os.path.join(\"data\", \"raw\")\nclean_path = os.path.join(\"data\", \"clean\")",
    "id": "b9256790c854440e8efb30c93972219a",
    "idx": 3,
    "time": "2021-01-25T19:46:35.680Z",
    "type": "execution"
   },
   {
    "id": "b9256790c854440e8efb30c93972219a",
    "time": "2021-01-25T19:46:35.763Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "idx": 4,
    "time": "2021-01-25T19:46:35.997Z",
    "type": "execution"
   },
   {
    "id": "f8710a185f8c466385d7d9827a9a914e",
    "time": "2021-01-25T19:46:36.265Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "idx": 5,
    "time": "2021-01-25T19:46:37.030Z",
    "type": "execution"
   },
   {
    "id": "4a4b9cb3876c420589f1e99261e50237",
    "time": "2021-01-25T19:46:37.122Z",
    "type": "completion"
   },
   {
    "code": "def downloadStations(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"ghcnd-stations.txt\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n        file = wget.download(url, out = out)\n    else:\n        print(\"ghcnd-stations.txt already exists\")\n\ndef downloadWeather(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"2020.csv\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz'\n        wget.download(url, out = out)\n\n        !gzip -d data/raw/2020.csv.gz\n    else:\n        print(\"2020.csv already exists\")",
    "id": "8d90e8d7f9b3499a8cad60900b10c1e7",
    "idx": 21,
    "time": "2021-01-25T19:48:52.122Z",
    "type": "execution"
   },
   {
    "id": "8d90e8d7f9b3499a8cad60900b10c1e7",
    "time": "2021-01-25T19:48:52.267Z",
    "type": "completion"
   },
   {
    "code": "def get_vals(line):\n    ls = line.split(',')\n    station = ls[0]\n    time = ls[1]\n    val = float(ls[3])\n    return [station, time, val]\n\ndef get_stations(filename = os.path.join(raw_path, 'ghcnd-stations.txt')):\n    df = pd.read_csv(filename, '/t', header=None)\n    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n    df_drop = df[df[\"Station\"].str.startswith(\"US\")].reset_index(drop = True)\n    return df_drop\n\ndef process_year(col='TAVG', path = raw_path):\n    tavg = []\n    pattern = re.compile(r\"^US.*TAVG\")\n    with open(os.path.join(path, \"2020.csv\")) as h:\n        l = h.readline()\n        while l:\n            if re.match(pattern, l) is not None:\n                v = get_vals(l)\n                tavg.append(get_vals(l))\n            l = h.readline()\n    df_tavg = pd.DataFrame(tavg, columns=['Station', 'Date', col])\n    return df_tavg\n\ndef mergeStationAggYear(stations, df_tavg):\n    merged = df_tavg.merge(stations, on='Station', how='inner')\n    \n    merged[\"TAVG\"] = merged[\"TAVG\"]/10 + 273\n\n    # Get mean temp and temp standard deviation\n    agg_df = merged.loc[:,[\"Station\",\"Latitude\", \"Longitude\", \"TAVG\"]].groupby([\"Station\", \"Latitude\", \"Longitude\"]).agg(\n        temp_mean = (\"TAVG\", np.mean),\n        temp_std = (\"TAVG\", np.std))\n\n    #Convert temperture mean to celsius for intepretability\n    agg_df[\"temp_mean\"] = agg_df[\"temp_mean\"] - 273\n    return agg_df",
    "id": "30857f217d6148c991286ac7bb102cfa",
    "idx": 22,
    "time": "2021-01-25T19:48:52.886Z",
    "type": "execution"
   },
   {
    "id": "30857f217d6148c991286ac7bb102cfa",
    "time": "2021-01-25T19:48:52.950Z",
    "type": "completion"
   },
   {
    "code": "def weatherToDf():\n    \"\"\"\n    The full pipeline to:\n    1. Download weather station temperature data (US only)\n    2. Merge station location to temp data\n    3. Aggregate mean temperature, and standard deviation across the year\n    \"\"\"\n    downloadStations()\n    print(\"Downloaded weather stations\")\n    downloadWeather()\n    print(\"Downloaded temperature data.\")\n    \n    stations = get_stations()\n    df_tavg = process_year(col='TAVG')\n    merged_agg = mergeStationAggYear(stations, df_tavg).reset_index()\n    \n    return merged_agg",
    "id": "e529c6e93e4f49c7bf6bde51ea18afa7",
    "idx": 23,
    "time": "2021-01-25T19:48:55.564Z",
    "type": "execution"
   },
   {
    "id": "e529c6e93e4f49c7bf6bde51ea18afa7",
    "time": "2021-01-25T19:48:55.620Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg = weatherToDf()",
    "id": "dc3b7d408bc94a0f9262d103420cd85b",
    "idx": 24,
    "time": "2021-01-25T19:49:07.445Z",
    "type": "execution"
   },
   {
    "id": "dc3b7d408bc94a0f9262d103420cd85b",
    "time": "2021-01-25T19:51:09.754Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "idx": 1,
    "time": "2021-01-25T19:55:02.836Z",
    "type": "execution"
   },
   {
    "id": "df402a9fcc69467b88e23fbeb32e29a9",
    "time": "2021-01-25T19:55:02.905Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg = weatherToDf()",
    "id": "dc3b7d408bc94a0f9262d103420cd85b",
    "idx": 24,
    "time": "2021-01-25T19:55:15.323Z",
    "type": "execution"
   },
   {
    "id": "dc3b7d408bc94a0f9262d103420cd85b",
    "time": "2021-01-25T19:56:06.236Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.describe()",
    "id": "b49c4370e47a43ae85d891e1af4543f3",
    "idx": 25,
    "time": "2021-01-25T19:56:12.172Z",
    "type": "execution"
   },
   {
    "id": "b49c4370e47a43ae85d891e1af4543f3",
    "time": "2021-01-25T19:56:12.287Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.head()",
    "id": "60e4d5750d2b4bba909faa028daca614",
    "idx": 25,
    "time": "2021-01-25T19:57:10.884Z",
    "type": "execution"
   },
   {
    "id": "60e4d5750d2b4bba909faa028daca614",
    "time": "2021-01-25T19:57:10.979Z",
    "type": "completion"
   },
   {
    "code": "#Reverse geocode latitutude and longitutes to county and state\n#Function to reverse_geocode counties\n\ndef addCountyStateFromLatLong(dataframe, lat, long, US_only = False):\n    \"\"\"\n    Takes a dataframe with latitude and longitude and adds the county and state.\n    \n    Inputs:\n    1) Dataframe \n    2) Latitude Column\n    3) Longitude Column\n    \n    Returns: \n    1) Dataframe with County and State\n    \"\"\"\n    \n    #Use lat and long as tuples to extract county and state \n    query = rg.search([tuple(x) for x in dataframe[[lat, long]].values])\n    \n    #Initialize lists to extract county and state from our query\n    state = []\n    county = []\n    country = []\n    \n    if US_only:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n            country.append(query[i][\"cc\"])\n    else:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n    \n    #Create dataframe of filled lists\n    if US_only:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county,\n                        \"country\": country})\n    else:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county})\n    \n    #assert that the query and station dataframes are the same size\n    assert len(dataframe) == len(sc_df), \"Row lengths don't match for input and output. Check lat/long values in input.\"\n    \n    #Merge the Station with county and state horizontally\n    if US_only:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n\n        concat_df = concat_df[concat_df[\"country\"] == \"US\"].drop(columns = \"country\")\n    else:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n    \n    return concat_df",
    "id": "7ea3f20691874c9481415b826d198fbb",
    "idx": 28,
    "time": "2021-01-25T20:00:22.338Z",
    "type": "execution"
   },
   {
    "id": "7ea3f20691874c9481415b826d198fbb",
    "time": "2021-01-25T20:00:22.407Z",
    "type": "completion"
   },
   {
    "code": "weather_station_state_county = addCountyStateFromLatLong(weather_station_agg, \"Latitude\", \"Longitude\", US_only=True)",
    "id": "69af4ecdfb76459782f2bbef5df0b3a1",
    "idx": 29,
    "time": "2021-01-25T20:00:27.040Z",
    "type": "execution"
   },
   {
    "id": "69af4ecdfb76459782f2bbef5df0b3a1",
    "time": "2021-01-25T20:00:28.355Z",
    "type": "completion"
   },
   {
    "code": "weather_station_state_county.head()",
    "id": "2ebc078619f34c65a99dc6ca82790ce5",
    "idx": 30,
    "time": "2021-01-25T20:00:45.918Z",
    "type": "execution"
   },
   {
    "id": "2ebc078619f34c65a99dc6ca82790ce5",
    "time": "2021-01-25T20:00:46.024Z",
    "type": "completion"
   },
   {
    "code": "def fipsIntToString(sr):\n    \"\"\"\n    Takes a series of FIPS integers with NA values\n    Returns a FIPS string 5 with 0 padding up front\n    \"\"\"\n    string = sr.fillna(0).astype(\"int32\").astype(\"str\")\n    padding = string.apply(lambda x: \"0\"*(5 - len(x)) + x)\n    return padding",
    "id": "1433bd3b2e52441c82a04cba78c5bb00",
    "idx": 31,
    "time": "2021-01-25T20:01:00.835Z",
    "type": "execution"
   },
   {
    "id": "1433bd3b2e52441c82a04cba78c5bb00",
    "time": "2021-01-25T20:01:00.907Z",
    "type": "completion"
   },
   {
    "code": "#Helper function to join county and state with FIPS from a lookup table\ndef add_FIPS(dataframe, path):\n    \"\"\"\n    This function joins FIPS codes to the state and county outputs of reverse_geocoder\n    Inputs:\n    1. Dataframe with \"state\" and \"county\" columns\n    2. Path to FIPS lookup table\n    \"\"\"\n    #Download FIPS lookuptable with relevant columns\n    lookuptable= pd.read_csv(path).loc[:,[\"FIPS\", \"Combined_Key\"]]\n    \n    #Add the FIPS codes\n    add_FIPS = dataframe.copy()\n    add_FIPS[\"Combined_Key\"] = add_FIPS[\"county\"].str.replace(\" County\", \", \") + add_FIPS[\"state\"] + \", US\"\n    climate_FIPS = add_FIPS.merge(lookuptable, on = \"Combined_Key\")\n    climate_FIPS.drop(columns = [\"state\", \"county\", \"Combined_Key\"], inplace = True)\n\n    #Convert FIPS to 5 digit string\n    climate_FIPS[\"FIPS\"] = fipsIntToString(climate_FIPS[\"FIPS\"])\n    return climate_FIPS",
    "id": "41c7bc514a794f428c632a7a6df8465a",
    "idx": 32,
    "time": "2021-01-25T20:01:01.196Z",
    "type": "execution"
   },
   {
    "id": "41c7bc514a794f428c632a7a6df8465a",
    "time": "2021-01-25T20:01:01.258Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "d00a22eb4ce343458daed1a5367a4a0a",
    "idx": 1,
    "time": "2021-01-26T19:49:45.121Z",
    "type": "execution"
   },
   {
    "id": "d00a22eb4ce343458daed1a5367a4a0a",
    "time": "2021-01-26T19:49:45.748Z",
    "type": "completion"
   },
   {
    "code": "!pip install reverse_geocoder\n!pip install wget\n!pip install reverse_geocoder\n!pip install kaleido",
    "id": "d4b4b2e0fd854e5998615328dbc9348b",
    "idx": 0,
    "time": "2021-01-26T19:49:57.470Z",
    "type": "execution"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "d00a22eb4ce343458daed1a5367a4a0a",
    "idx": 1,
    "time": "2021-01-26T19:50:13.061Z",
    "type": "execution"
   },
   {
    "id": "d4b4b2e0fd854e5998615328dbc9348b",
    "time": "2021-01-26T19:50:16.870Z",
    "type": "completion"
   },
   {
    "id": "d00a22eb4ce343458daed1a5367a4a0a",
    "time": "2021-01-26T19:50:17.904Z",
    "type": "completion"
   },
   {
    "code": "pd.set_option('display.max_rows', 200)\npd.set_option('display.max_colwidth', -1)",
    "id": "d5f12b870a3b423199ac94eee93136c4",
    "idx": 2,
    "time": "2021-01-26T19:50:25.774Z",
    "type": "execution"
   },
   {
    "id": "d5f12b870a3b423199ac94eee93136c4",
    "time": "2021-01-26T19:50:25.870Z",
    "type": "completion"
   },
   {
    "code": "raw_path = os.path.join(\"data\", \"raw\")\nclean_path = os.path.join(\"data\", \"clean\")",
    "id": "143cc64235fa4a788889247bf1e25931",
    "idx": 3,
    "time": "2021-01-26T19:50:26.044Z",
    "type": "execution"
   },
   {
    "id": "143cc64235fa4a788889247bf1e25931",
    "time": "2021-01-26T19:50:26.147Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "1b5090192c644333bd3af1c3724c0d98",
    "idx": 4,
    "time": "2021-01-26T19:50:27.079Z",
    "type": "execution"
   },
   {
    "id": "1b5090192c644333bd3af1c3724c0d98",
    "time": "2021-01-26T19:50:27.144Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "3b4c279d470b4aa382c41938f510ec6e",
    "idx": 5,
    "time": "2021-01-26T19:50:27.619Z",
    "type": "execution"
   },
   {
    "id": "3b4c279d470b4aa382c41938f510ec6e",
    "time": "2021-01-26T19:50:27.724Z",
    "type": "completion"
   },
   {
    "code": "def censusApiToDf(year_estimate = \"acs5\", features = False, metadata_only = False, geography = \"county:*&in=state:*\"):\n    \"\"\"\n    Takes parameters to extract data for the ACS 2019 DP03 and DP05 datasets.\n    Returns a dataframe of ACS features or variables at the county level.\n    \n    Allows the following parameters to personalize query:\n    \n    1. year_estimate - Choose between acs1 and acs5 for 1-year and 5-year estimates.\n        1-year estimates has more current, but less data.\n        5-year estimates has less current, but more data.\n        \n    2. features - A list of features to extract from ACS data.\n    \n    3. metadata_only - If True, returns the metadata of features for a given datatable.\n    \n    4. geography - Specify US locations to extract. Defaults to extracting all counties.\n    \"\"\"\n    \n    baseAPI = f\"https://api.census.gov/data/2019/acs/{year_estimate}\"\n    \n    \n    if metadata_only:\n        url = f\"{baseAPI}/profile/variables\"\n        \n    elif features:\n        features = \",\".join(features)\n        url = f\"{baseAPI}/profile?get=NAME,{features}&for={geography}\"\n        \n    else:\n        return print(\"Error: Must either set variables = True, or pass input features to extract.\")\n\n    response = requests.get(url)\n    jsonResponse = json.loads(response.text)\n    \n    raw = pd.DataFrame(data= jsonResponse)\n    headers = raw.iloc[0]\n    raw.columns = headers\n    df = raw.loc[1:,:]\n    \n    return df",
    "id": "b53dde55d9b1473e8175316738a5c638",
    "idx": 7,
    "time": "2021-01-26T19:50:29.419Z",
    "type": "execution"
   },
   {
    "id": "b53dde55d9b1473e8175316738a5c638",
    "time": "2021-01-26T19:50:29.479Z",
    "type": "completion"
   },
   {
    "code": "census_metadata = censusApiToDf(metadata_only = True).loc[4:, [\"name\", \"label\"]]",
    "id": "65e8bc30fef34f90a6581035d360bd8b",
    "idx": 8,
    "time": "2021-01-26T19:50:29.600Z",
    "type": "execution"
   },
   {
    "code": "census_metadata.sample(frac=1, random_state=8).head()",
    "id": "1e24c1747bb249a98fa6b19fd81b0acb",
    "idx": 9,
    "time": "2021-01-26T19:50:29.802Z",
    "type": "execution"
   },
   {
    "id": "65e8bc30fef34f90a6581035d360bd8b",
    "time": "2021-01-26T19:50:30.099Z",
    "type": "completion"
   },
   {
    "id": "1e24c1747bb249a98fa6b19fd81b0acb",
    "time": "2021-01-26T19:50:30.113Z",
    "type": "completion"
   },
   {
    "code": "def splitCensusMetadata(df):\n    \"\"\"\n    Takes the raw census metdata dataframe and splits the name column by the \"!!\" delimiter\n    \"\"\"\n    split = df.join(df['label'].str.split(\"!!\", expand=True))\n    return split",
    "id": "569a98027dd0458f8f3382481051bebf",
    "idx": 11,
    "time": "2021-01-26T19:50:30.197Z",
    "type": "execution"
   },
   {
    "id": "569a98027dd0458f8f3382481051bebf",
    "time": "2021-01-26T19:50:30.272Z",
    "type": "completion"
   },
   {
    "code": "splitCensusMetadata(census_metadata).sample(frac=1, random_state=8).head()",
    "id": "2e83dfc12d9844ba8c1e9e750d7913d3",
    "idx": 12,
    "time": "2021-01-26T19:50:30.747Z",
    "type": "execution"
   },
   {
    "id": "2e83dfc12d9844ba8c1e9e750d7913d3",
    "time": "2021-01-26T19:50:30.843Z",
    "type": "completion"
   },
   {
    "code": "def findPatterns(df, col, patterns):\n    \"\"\"\n    Takes a dataframe, specified columns, and a list of string patterns.\n    Returns a dataframe with matches to all strings patterns.\n    \"\"\"\n\n    string_masks = (df[col].str.contains(string, regex = True, case = False) for string in patterns)\n    comb_mask = np.vstack(string_masks).all(axis=0)\n    final_df = df[comb_mask]\n\n    return final_df",
    "id": "66523c252e8647ac94999f18739d4c51",
    "idx": 14,
    "time": "2021-01-26T19:50:31.826Z",
    "type": "execution"
   },
   {
    "id": "66523c252e8647ac94999f18739d4c51",
    "time": "2021-01-26T19:50:31.889Z",
    "type": "completion"
   },
   {
    "code": "race_ethnicity_cols = findPatterns(df = census_metadata,\n                    col = \"label\",\n                    patterns= [\"percent!!\", \n                    \"total\", \n                    \"one race|hispanic or latino \\(of any race\\)$\", \n                    \"black|white|pacific islander$|asian$|hispanic\"])\n\nfiltered_ethnicity_cols = race_ethnicity_cols[~race_ethnicity_cols[\"label\"].str.contains(\"Other Asian|!!Other Pacific Islander\")]\nDP05_cols = filtered_ethnicity_cols[\"name\"].values\nfiltered_ethnicity_cols",
    "id": "8b83e39fbdd04650822976459383f291",
    "idx": 15,
    "time": "2021-01-26T19:50:31.984Z",
    "type": "execution"
   },
   {
    "id": "8b83e39fbdd04650822976459383f291",
    "time": "2021-01-26T19:50:32.078Z",
    "type": "completion"
   },
   {
    "code": "income_pat = [\"estimate!!\", \"income\", \"median household\"]\ninsurance_pat = [\"percent!!\", \"with health insurance coverage$\", \"noninstitutionalized population!!\"]\npoverty_pat = [\"percent!!\", \"below the poverty level\", \"all families$\"]\n\neconomic_cols = pd.concat([findPatterns(census_metadata, \"label\", income_pat),\n                          findPatterns(census_metadata, \"label\", insurance_pat),\n                          findPatterns(census_metadata, \"label\", poverty_pat)])\n\nDP03_cols = economic_cols[\"name\"].values\neconomic_cols",
    "id": "735057442e1b44e38a2d0c32b47da1cd",
    "idx": 17,
    "time": "2021-01-26T19:50:32.389Z",
    "type": "execution"
   },
   {
    "id": "735057442e1b44e38a2d0c32b47da1cd",
    "time": "2021-01-26T19:50:32.694Z",
    "type": "completion"
   },
   {
    "code": "census_features_all = list(DP03_cols) + list(DP05_cols)\ncensus_colnames = {\"DP05_0037PE\": \"white_perc\",\n                    \"DP05_0038PE\": \"black_perc\",\n                    \"DP05_0039PE\": \"amer_native_perc\",\n                    \"DP05_0044PE\": \"asian_perc\",\n                    \"DP05_0052PE\": \"pacific_perc\",\n                    \"DP05_0071PE\": \"hispanic_perc\",\n                    \"DP03_0062E\": \"income_med_dollars\",\n                    \"DP03_0096PE\": \"insurance_perc\",\n                    \"DP03_0119PE\": \"poverty_perc\"}\n\ncensus_fips = censusApiToDf(year_estimate=\"acs1\", features= census_features_all).rename(columns=census_colnames)\ncensus_fips[\"FIPS\"] = census_fips[\"state\"] + census_fips[\"county\"]\ncensus_fips.drop(columns = [\"state\", \"county\"], inplace= True)\n\ncensus_fips",
    "id": "c11c897147f24a6b828a9132c962fc35",
    "idx": 19,
    "time": "2021-01-26T19:50:34.488Z",
    "type": "execution"
   },
   {
    "code": "def downloadStations(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"ghcnd-stations.txt\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n        file = wget.download(url, out = out)\n    else:\n        print(\"ghcnd-stations.txt already exists\")\n\ndef downloadWeather(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"2020.csv\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz'\n        wget.download(url, out = out)\n\n        !gzip -d data/raw/2020.csv.gz\n    else:\n        print(\"2020.csv already exists\")",
    "id": "de4e6b5ad19b453b8792a0cc9ff8065f",
    "idx": 21,
    "time": "2021-01-26T19:50:35.404Z",
    "type": "execution"
   },
   {
    "code": "def get_vals(line):\n    ls = line.split(',')\n    station = ls[0]\n    time = ls[1]\n    val = float(ls[3])\n    return [station, time, val]\n\ndef get_stations(filename = os.path.join(raw_path, 'ghcnd-stations.txt')):\n    df = pd.read_csv(filename, '/t', header=None)\n    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n    df_drop = df[df[\"Station\"].str.startswith(\"US\")].reset_index(drop = True)\n    return df_drop\n\ndef process_year(col='TAVG', path = raw_path):\n    tavg = []\n    pattern = re.compile(r\"^US.*TAVG\")\n    with open(os.path.join(path, \"2020.csv\")) as h:\n        l = h.readline()\n        while l:\n            if re.match(pattern, l) is not None:\n                v = get_vals(l)\n                tavg.append(get_vals(l))\n            l = h.readline()\n    df_tavg = pd.DataFrame(tavg, columns=['Station', 'Date', col])\n    return df_tavg\n\ndef mergeStationAggYear(stations, df_tavg):\n    merged = df_tavg.merge(stations, on='Station', how='inner')\n    \n    merged[\"TAVG\"] = merged[\"TAVG\"]/10 + 273\n\n    # Get mean temp and temp standard deviation across year\n    agg_df = merged.loc[:,[\"Station\",\"Latitude\", \"Longitude\", \"TAVG\"]].groupby([\"Station\", \"Latitude\", \"Longitude\"]).agg(\n        temp_mean = (\"TAVG\", np.mean),\n        temp_std = (\"TAVG\", np.std))\n\n    #Convert temperture mean to celsius for intepretability\n    agg_df[\"temp_mean\"] = agg_df[\"temp_mean\"] - 273\n    return agg_df",
    "id": "e34441b21fa34f59850de35550c128fc",
    "idx": 22,
    "time": "2021-01-26T19:50:35.640Z",
    "type": "execution"
   },
   {
    "id": "c11c897147f24a6b828a9132c962fc35",
    "time": "2021-01-26T19:50:35.719Z",
    "type": "completion"
   },
   {
    "id": "de4e6b5ad19b453b8792a0cc9ff8065f",
    "time": "2021-01-26T19:50:35.721Z",
    "type": "completion"
   },
   {
    "id": "e34441b21fa34f59850de35550c128fc",
    "time": "2021-01-26T19:50:35.734Z",
    "type": "completion"
   },
   {
    "code": "def weatherToDf():\n    \"\"\"\n    The full pipeline to:\n    1. Download weather station temperature data (US only)\n    2. Merge station location to temp data\n    3. Aggregate mean temperature, and standard deviation across the year\n    \"\"\"\n    downloadStations()\n    print(\"Downloaded weather stations\")\n    downloadWeather()\n    print(\"Downloaded temperature data.\")\n    \n    stations = get_stations()\n    df_tavg = process_year(col='TAVG')\n    merged_agg = mergeStationAggYear(stations, df_tavg).reset_index()\n    \n    return merged_agg",
    "id": "a92e566015ac4e9f8c256616080a91dc",
    "idx": 23,
    "time": "2021-01-26T19:50:35.785Z",
    "type": "execution"
   },
   {
    "id": "a92e566015ac4e9f8c256616080a91dc",
    "time": "2021-01-26T19:50:35.838Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg = weatherToDf()",
    "id": "1035a51d1af542d18cd422a39017f6a6",
    "idx": 24,
    "time": "2021-01-26T19:50:35.945Z",
    "type": "execution"
   },
   {
    "code": "weather_station_agg.head()",
    "id": "2556faf28d5f46ebb2c0cf5613924716",
    "idx": 25,
    "time": "2021-01-26T19:50:36.146Z",
    "type": "execution"
   },
   {
    "code": "weather_station_agg.describe()",
    "id": "feade9b8b939452080abe77aa815444a",
    "idx": 26,
    "time": "2021-01-26T19:50:36.754Z",
    "type": "execution"
   },
   {
    "id": "1035a51d1af542d18cd422a39017f6a6",
    "time": "2021-01-26T19:51:28.472Z",
    "type": "completion"
   },
   {
    "id": "2556faf28d5f46ebb2c0cf5613924716",
    "time": "2021-01-26T19:51:28.555Z",
    "type": "completion"
   },
   {
    "id": "feade9b8b939452080abe77aa815444a",
    "time": "2021-01-26T19:51:28.594Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "2e027558b5d44f5eb70e209bf812b807",
    "idx": 1,
    "time": "2021-01-27T00:40:21.736Z",
    "type": "execution"
   },
   {
    "id": "2e027558b5d44f5eb70e209bf812b807",
    "time": "2021-01-27T00:40:23.145Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.head()",
    "id": "3dfde6a1940345a2b6e84f1e846799e6",
    "idx": 25,
    "time": "2021-01-27T00:40:32.537Z",
    "type": "execution"
   },
   {
    "id": "3dfde6a1940345a2b6e84f1e846799e6",
    "time": "2021-01-27T00:40:32.798Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "2e027558b5d44f5eb70e209bf812b807",
    "idx": 1,
    "time": "2021-01-27T00:40:36.812Z",
    "type": "execution"
   },
   {
    "code": "pd.set_option('display.max_rows', 200)\npd.set_option('display.max_colwidth', -1)",
    "id": "f849213a02094ff9910896971ccac0f1",
    "idx": 2,
    "time": "2021-01-27T00:40:36.991Z",
    "type": "execution"
   },
   {
    "id": "2e027558b5d44f5eb70e209bf812b807",
    "time": "2021-01-27T00:40:37.027Z",
    "type": "completion"
   },
   {
    "code": "raw_path = os.path.join(\"data\", \"raw\")\nclean_path = os.path.join(\"data\", \"clean\")",
    "id": "52b2c16040094da38b36561fbd703dab",
    "idx": 3,
    "time": "2021-01-27T00:40:37.170Z",
    "type": "execution"
   },
   {
    "id": "f849213a02094ff9910896971ccac0f1",
    "time": "2021-01-27T00:40:37.295Z",
    "type": "completion"
   },
   {
    "id": "52b2c16040094da38b36561fbd703dab",
    "time": "2021-01-27T00:40:37.298Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "6cf5ef1c5e744fddad8f893661feb51e",
    "idx": 4,
    "time": "2021-01-27T00:40:37.328Z",
    "type": "execution"
   },
   {
    "id": "6cf5ef1c5e744fddad8f893661feb51e",
    "time": "2021-01-27T00:40:37.409Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "15cd4d1d15f0486b944c6e7352d8b483",
    "idx": 5,
    "time": "2021-01-27T00:40:37.485Z",
    "type": "execution"
   },
   {
    "id": "15cd4d1d15f0486b944c6e7352d8b483",
    "time": "2021-01-27T00:40:37.597Z",
    "type": "completion"
   },
   {
    "code": "def censusApiToDf(year_estimate = \"acs5\", features = False, metadata_only = False, geography = \"county:*&in=state:*\"):\n    \"\"\"\n    Takes parameters to extract data for the ACS 2019 DP03 and DP05 datasets.\n    Returns a dataframe of ACS features or variables at the county level.\n    \n    Allows the following parameters to personalize query:\n    \n    1. year_estimate - Choose between acs1 and acs5 for 1-year and 5-year estimates.\n        1-year estimates has more current, but less data.\n        5-year estimates has less current, but more data.\n        \n    2. features - A list of features to extract from ACS data.\n    \n    3. metadata_only - If True, returns the metadata of features for a given datatable.\n    \n    4. geography - Specify US locations to extract. Defaults to extracting all counties.\n    \"\"\"\n    \n    baseAPI = f\"https://api.census.gov/data/2019/acs/{year_estimate}\"\n    \n    \n    if metadata_only:\n        url = f\"{baseAPI}/profile/variables\"\n        \n    elif features:\n        features = \",\".join(features)\n        url = f\"{baseAPI}/profile?get=NAME,{features}&for={geography}\"\n        \n    else:\n        return print(\"Error: Must either set variables = True, or pass input features to extract.\")\n\n    response = requests.get(url)\n    jsonResponse = json.loads(response.text)\n    \n    raw = pd.DataFrame(data= jsonResponse)\n    headers = raw.iloc[0]\n    raw.columns = headers\n    df = raw.loc[1:,:]\n    \n    return df",
    "id": "3967801d60194d23816d47e60ec29a85",
    "idx": 7,
    "time": "2021-01-27T00:40:37.805Z",
    "type": "execution"
   },
   {
    "id": "3967801d60194d23816d47e60ec29a85",
    "time": "2021-01-27T00:40:37.890Z",
    "type": "completion"
   },
   {
    "code": "census_metadata = censusApiToDf(metadata_only = True).loc[4:, [\"name\", \"label\"]]",
    "id": "ca05f52bff9b44528b7d72308fada03f",
    "idx": 8,
    "time": "2021-01-27T00:40:37.982Z",
    "type": "execution"
   },
   {
    "code": "census_metadata.sample(frac=1, random_state=8).head()",
    "id": "8939762a3d2f4e009ff0f2bb86b0bb63",
    "idx": 9,
    "time": "2021-01-27T00:40:38.139Z",
    "type": "execution"
   },
   {
    "id": "ca05f52bff9b44528b7d72308fada03f",
    "time": "2021-01-27T00:40:38.401Z",
    "type": "completion"
   },
   {
    "code": "def splitCensusMetadata(df):\n    \"\"\"\n    Takes the raw census metdata dataframe and splits the name column by the \"!!\" delimiter\n    \"\"\"\n    split = df.join(df['label'].str.split(\"!!\", expand=True))\n    return split",
    "id": "1e853e8515f14f69a96b249de5928bbb",
    "idx": 11,
    "time": "2021-01-27T00:40:38.454Z",
    "type": "execution"
   },
   {
    "id": "8939762a3d2f4e009ff0f2bb86b0bb63",
    "time": "2021-01-27T00:40:38.560Z",
    "type": "completion"
   },
   {
    "id": "1e853e8515f14f69a96b249de5928bbb",
    "time": "2021-01-27T00:40:38.585Z",
    "type": "completion"
   },
   {
    "code": "splitCensusMetadata(census_metadata).sample(frac=1, random_state=8).head()",
    "id": "d89fdb13c5f74801937a8d79f5668bd5",
    "idx": 12,
    "time": "2021-01-27T00:40:38.635Z",
    "type": "execution"
   },
   {
    "id": "d89fdb13c5f74801937a8d79f5668bd5",
    "time": "2021-01-27T00:40:38.928Z",
    "type": "completion"
   },
   {
    "code": "def findPatterns(df, col, patterns):\n    \"\"\"\n    Takes a dataframe, specified columns, and a list of string patterns.\n    Returns a dataframe with matches to all strings patterns.\n    \"\"\"\n\n    string_masks = (df[col].str.contains(string, regex = True, case = False) for string in patterns)\n    comb_mask = np.vstack(string_masks).all(axis=0)\n    final_df = df[comb_mask]\n\n    return final_df",
    "id": "0543885463f349eab90d273b03276c95",
    "idx": 14,
    "time": "2021-01-27T00:40:39.262Z",
    "type": "execution"
   },
   {
    "id": "0543885463f349eab90d273b03276c95",
    "time": "2021-01-27T00:40:39.341Z",
    "type": "completion"
   },
   {
    "code": "race_ethnicity_cols = findPatterns(df = census_metadata,\n                    col = \"label\",\n                    patterns= [\"percent!!\", \n                    \"total\", \n                    \"one race|hispanic or latino \\(of any race\\)$\", \n                    \"black|white|pacific islander$|asian$|hispanic\"])\n\nfiltered_ethnicity_cols = race_ethnicity_cols[~race_ethnicity_cols[\"label\"].str.contains(\"Other Asian|!!Other Pacific Islander\")]\nDP05_cols = filtered_ethnicity_cols[\"name\"].values\nfiltered_ethnicity_cols",
    "id": "f6dca1fc1a694c3b89e958fce8eb795d",
    "idx": 15,
    "time": "2021-01-27T00:40:39.443Z",
    "type": "execution"
   },
   {
    "id": "f6dca1fc1a694c3b89e958fce8eb795d",
    "time": "2021-01-27T00:40:39.559Z",
    "type": "completion"
   },
   {
    "code": "income_pat = [\"estimate!!\", \"income\", \"median household\"]\ninsurance_pat = [\"percent!!\", \"with health insurance coverage$\", \"noninstitutionalized population!!\"]\npoverty_pat = [\"percent!!\", \"below the poverty level\", \"all families$\"]\n\neconomic_cols = pd.concat([findPatterns(census_metadata, \"label\", income_pat),\n                          findPatterns(census_metadata, \"label\", insurance_pat),\n                          findPatterns(census_metadata, \"label\", poverty_pat)])\n\nDP03_cols = economic_cols[\"name\"].values\neconomic_cols",
    "id": "640a8d8ed84d47b39e4bb24ecd82338a",
    "idx": 17,
    "time": "2021-01-27T00:40:40.117Z",
    "type": "execution"
   },
   {
    "id": "640a8d8ed84d47b39e4bb24ecd82338a",
    "time": "2021-01-27T00:40:40.225Z",
    "type": "completion"
   },
   {
    "code": "census_features_all = list(DP03_cols) + list(DP05_cols)\ncensus_colnames = {\"DP05_0037PE\": \"white_perc\",\n                    \"DP05_0038PE\": \"black_perc\",\n                    \"DP05_0039PE\": \"amer_native_perc\",\n                    \"DP05_0044PE\": \"asian_perc\",\n                    \"DP05_0052PE\": \"pacific_perc\",\n                    \"DP05_0071PE\": \"hispanic_perc\",\n                    \"DP03_0062E\": \"income_med_dollars\",\n                    \"DP03_0096PE\": \"insurance_perc\",\n                    \"DP03_0119PE\": \"poverty_perc\"}\n\ncensus_fips = censusApiToDf(year_estimate=\"acs1\", features= census_features_all).rename(columns=census_colnames)\ncensus_fips[\"FIPS\"] = census_fips[\"state\"] + census_fips[\"county\"]\ncensus_fips.drop(columns = [\"state\", \"county\"], inplace= True)\n\ncensus_fips",
    "id": "2c45ef64e9144e31820543b312e1689e",
    "idx": 19,
    "time": "2021-01-27T00:40:40.404Z",
    "type": "execution"
   },
   {
    "code": "def downloadStations(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"ghcnd-stations.txt\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n        file = wget.download(url, out = out)\n    else:\n        print(\"ghcnd-stations.txt already exists\")\n\ndef downloadWeather(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"2020.csv\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz'\n        wget.download(url, out = out)\n\n        !gzip -d data/raw/2020.csv.gz\n    else:\n        print(\"2020.csv already exists\")",
    "id": "31f64c1110824011bff43c2bacee33c8",
    "idx": 21,
    "time": "2021-01-27T00:40:40.749Z",
    "type": "execution"
   },
   {
    "code": "def get_vals(line):\n    ls = line.split(',')\n    station = ls[0]\n    time = ls[1]\n    val = float(ls[3])\n    return [station, time, val]\n\ndef get_stations(filename = os.path.join(raw_path, 'ghcnd-stations.txt')):\n    df = pd.read_csv(filename, '/t', header=None)\n    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n    df_drop = df[df[\"Station\"].str.startswith(\"US\")].reset_index(drop = True)\n    return df_drop\n\ndef process_year(col='TAVG', path = raw_path):\n    tavg = []\n    pattern = re.compile(r\"^US.*TAVG\")\n    with open(os.path.join(path, \"2020.csv\")) as h:\n        l = h.readline()\n        while l:\n            if re.match(pattern, l) is not None:\n                v = get_vals(l)\n                tavg.append(get_vals(l))\n            l = h.readline()\n    df_tavg = pd.DataFrame(tavg, columns=['Station', 'Date', col])\n    return df_tavg\n\ndef mergeStationAggYear(stations, df_tavg):\n    merged = df_tavg.merge(stations, on='Station', how='inner')\n    \n    merged[\"TAVG\"] = merged[\"TAVG\"]/10 + 273\n\n    # Get mean temp and temp standard deviation across year\n    agg_df = merged.loc[:,[\"Station\",\"Latitude\", \"Longitude\", \"TAVG\"]].groupby([\"Station\", \"Latitude\", \"Longitude\"]).agg(\n        temp_mean = (\"TAVG\", np.mean),\n        temp_std = (\"TAVG\", np.std))\n\n    #Convert temperture mean to celsius for intepretability\n    agg_df[\"temp_mean\"] = agg_df[\"temp_mean\"] - 273\n    return agg_df",
    "id": "663d6ad1878140f285ad2c97cda6af82",
    "idx": 22,
    "time": "2021-01-27T00:40:41.109Z",
    "type": "execution"
   },
   {
    "code": "def weatherToDf():\n    \"\"\"\n    The full pipeline to:\n    1. Download weather station temperature data (US only)\n    2. Merge station location to temp data\n    3. Aggregate mean temperature, and standard deviation across the year\n    \"\"\"\n    downloadStations()\n    print(\"Downloaded weather stations\")\n    downloadWeather()\n    print(\"Downloaded temperature data.\")\n    \n    stations = get_stations()\n    df_tavg = process_year(col='TAVG')\n    merged_agg = mergeStationAggYear(stations, df_tavg).reset_index()\n    \n    return merged_agg",
    "id": "8dd66958c32e453c88be7df0c054abf0",
    "idx": 23,
    "time": "2021-01-27T00:40:41.287Z",
    "type": "execution"
   },
   {
    "id": "2c45ef64e9144e31820543b312e1689e",
    "time": "2021-01-27T00:40:41.865Z",
    "type": "completion"
   },
   {
    "id": "31f64c1110824011bff43c2bacee33c8",
    "time": "2021-01-27T00:40:41.877Z",
    "type": "completion"
   },
   {
    "id": "663d6ad1878140f285ad2c97cda6af82",
    "time": "2021-01-27T00:40:41.880Z",
    "type": "completion"
   },
   {
    "id": "8dd66958c32e453c88be7df0c054abf0",
    "time": "2021-01-27T00:40:41.882Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg = weatherToDf()",
    "id": "5aca170de124464087ab73326833aa19",
    "idx": 24,
    "time": "2021-01-27T00:40:43.811Z",
    "type": "execution"
   },
   {
    "id": "5aca170de124464087ab73326833aa19",
    "time": "2021-01-27T00:41:35.023Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.head()",
    "id": "3dfde6a1940345a2b6e84f1e846799e6",
    "idx": 25,
    "time": "2021-01-27T00:41:57.745Z",
    "type": "execution"
   },
   {
    "id": "3dfde6a1940345a2b6e84f1e846799e6",
    "time": "2021-01-27T00:41:57.854Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.describe()",
    "id": "1f9240c9c1f8409487c55ebf6dbd2d0c",
    "idx": 26,
    "time": "2021-01-27T00:42:00.174Z",
    "type": "execution"
   },
   {
    "id": "1f9240c9c1f8409487c55ebf6dbd2d0c",
    "time": "2021-01-27T00:42:00.274Z",
    "type": "completion"
   },
   {
    "code": "#Reverse geocode latitutude and longitutes to county and state\n#Function to reverse_geocode counties\n\ndef addCountyStateFromLatLong(dataframe, lat, long, US_only = False):\n    \"\"\"\n    Takes a dataframe with latitude and longitude and adds the county and state.\n    \n    Inputs:\n    1) Dataframe \n    2) Latitude Column\n    3) Longitude Column\n    \n    Returns: \n    1) Dataframe with County and State\n    \"\"\"\n    \n    #Use lat and long as tuples to extract county and state \n    query = rg.search([tuple(x) for x in dataframe[[lat, long]].values])\n    \n    #Initialize lists to extract county and state from our query\n    state = []\n    county = []\n    country = []\n    \n    if US_only:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n            country.append(query[i][\"cc\"])\n    else:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n    \n    #Create dataframe of filled lists\n    if US_only:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county,\n                        \"country\": country})\n    else:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county})\n    \n    #assert that the query and station dataframes are the same size\n    assert len(dataframe) == len(sc_df), \"Row lengths don't match for input and output. Check lat/long values in input.\"\n    \n    #Merge the Station with county and state horizontally\n    if US_only:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n\n        concat_df = concat_df[concat_df[\"country\"] == \"US\"].drop(columns = \"country\")\n    else:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n    \n    return concat_df",
    "id": "5283b2e7d41a4d8d8641283f9347309b",
    "idx": 28,
    "time": "2021-01-27T00:44:18.864Z",
    "type": "execution"
   },
   {
    "id": "5283b2e7d41a4d8d8641283f9347309b",
    "time": "2021-01-27T00:44:19.009Z",
    "type": "completion"
   },
   {
    "code": "weather_station_state_county = addCountyStateFromLatLong(weather_station_agg, \"Latitude\", \"Longitude\", US_only=True)",
    "id": "052dcf6d98404df8893f3e98ac5e0442",
    "idx": 29,
    "time": "2021-01-27T00:44:19.878Z",
    "type": "execution"
   },
   {
    "id": "052dcf6d98404df8893f3e98ac5e0442",
    "time": "2021-01-27T00:44:21.114Z",
    "type": "completion"
   },
   {
    "code": "weather_station_state_county.head()",
    "id": "ef198cc62d7f470ea771b24da2cf4e1a",
    "idx": 30,
    "time": "2021-01-27T00:44:24.197Z",
    "type": "execution"
   },
   {
    "id": "ef198cc62d7f470ea771b24da2cf4e1a",
    "time": "2021-01-27T00:44:24.315Z",
    "type": "completion"
   },
   {
    "code": "def fipsIntToString(sr):\n    \"\"\"\n    Takes a series of FIPS integers with NA values\n    Returns a FIPS string 5 with 0 padding up front\n    \"\"\"\n    string = sr.fillna(0).astype(\"int32\").astype(\"str\")\n    padding = string.apply(lambda x: \"0\"*(5 - len(x)) + x)\n    return padding",
    "id": "97787c6844d843e58f358866ea6d00d5",
    "idx": 34,
    "time": "2021-01-27T00:55:50.386Z",
    "type": "execution"
   },
   {
    "id": "97787c6844d843e58f358866ea6d00d5",
    "time": "2021-01-27T00:55:50.477Z",
    "type": "completion"
   },
   {
    "code": "#Helper function to join county and state with FIPS from a lookup table\ndef add_FIPS(dataframe):\n    \"\"\"\n    This function joins FIPS codes to the state and county outputs of reverse_geocoder\n    Inputs:\n    1. Dataframe with \"state\" and \"county\" columns\n    2. Path to FIPS lookup table\n    \"\"\"\n    #Download FIPS lookuptable with relevant columns\n    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\"\n    lookuptable= pd.read_csv(url).loc[:,[\"FIPS\", \"Combined_Key\"]]\n    \n    #Add the FIPS codes\n    add_FIPS = dataframe.copy()\n    add_FIPS[\"Combined_Key\"] = add_FIPS[\"county\"].str.replace(\" County\", \", \") + add_FIPS[\"state\"] + \", US\"\n    climate_FIPS = add_FIPS.merge(lookuptable, on = \"Combined_Key\")\n    climate_FIPS.drop(columns = [\"state\", \"county\", \"Combined_Key\"], inplace = True)\n\n    #Convert FIPS to 5 digit string\n    climate_FIPS[\"FIPS\"] = fipsIntToString(climate_FIPS[\"FIPS\"])\n    return climate_FIPS",
    "id": "e893d6394e154659aecd9a1afbb43b40",
    "idx": 35,
    "time": "2021-01-27T00:55:54.076Z",
    "type": "execution"
   },
   {
    "id": "e893d6394e154659aecd9a1afbb43b40",
    "time": "2021-01-27T00:55:54.332Z",
    "type": "completion"
   },
   {
    "code": "weather_station_FIPS = add_FIPS(weather_station_state_county)\nweather_FIPS_agg =  weather_station_FIPS.groupby(\"FIPS\").agg(temp_mean = (\"temp_mean\", np.mean),\n                                                            temp_range = (\"temp_range\", np.mean),\n                                                            temp_std = (\"temp_std\", np.mean),\n                                                            precip_mean = (\"precip_mean\", np.mean)).reset_index()",
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "idx": 37,
    "time": "2021-01-27T00:56:05.800Z",
    "type": "execution"
   },
   {
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "time": "2021-01-27T00:56:06.384Z",
    "type": "completion"
   },
   {
    "code": "weather_station_FIPS = add_FIPS(weather_station_state_county)\nweather_FIPS_agg =  weather_station_FIPS.groupby(\"FIPS\").agg(temp_mean = (\"temp_mean\", np.mean),\n                                                            temp_std = (\"temp_std\", np.mean),\n                                                            precip_mean = (\"precip_mean\", np.mean)).reset_index()",
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "idx": 37,
    "time": "2021-01-27T00:56:25.403Z",
    "type": "execution"
   },
   {
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "time": "2021-01-27T00:56:25.673Z",
    "type": "completion"
   },
   {
    "code": "weather_station_FIPS = add_FIPS(weather_station_state_county)\nweather_FIPS_agg =  weather_station_FIPS.groupby(\"FIPS\").agg(temp_mean = (\"temp_mean\", np.mean),\n                                                            temp_std = (\"temp_std\", np.mean)).reset_index()",
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "idx": 37,
    "time": "2021-01-27T00:56:33.925Z",
    "type": "execution"
   },
   {
    "id": "5853f32ac51749ce8004ccaacd34634c",
    "time": "2021-01-27T00:56:34.136Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPS_agg.head()",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T00:56:35.758Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T00:56:35.883Z",
    "type": "completion"
   },
   {
    "code": "def downloadCOVID(dir=\"Data/Raw\"):\n    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/12-31-2020.csv\"\n    \n    file = url.split(\"/\")[-1]\n\n    if os.path.exists(dir):\n        if os.path.exists(os.path.join(dir, file)):\n            print(file, \"already exists\")\n        else:\n          wget.download(url=url, out=dir)\n    else:\n        print(f\"The directory {dir} does not exist\")\n    ",
    "id": "1f7b3882ef354969b593ed11f9e8cd50",
    "idx": 40,
    "time": "2021-01-27T00:56:51.542Z",
    "type": "execution"
   },
   {
    "id": "1f7b3882ef354969b593ed11f9e8cd50",
    "time": "2021-01-27T00:56:51.708Z",
    "type": "completion"
   },
   {
    "code": "downloadCOVID()",
    "id": "750a1d51134a4bd08f3c3078040cee72",
    "idx": 41,
    "time": "2021-01-27T00:56:54.017Z",
    "type": "execution"
   },
   {
    "id": "750a1d51134a4bd08f3c3078040cee72",
    "time": "2021-01-27T00:56:54.184Z",
    "type": "completion"
   },
   {
    "code": "def covidUsDf(path = \"Data/Raw\", file = \"12-31-2020.csv\"):\n    \"\"\"\n    Takes a path and a file containing data from the CSSE COVID-19 Daily Reports\n\n    Returns a dataframe with only US data\n    \"\"\"\n    raw = pd.read_csv(os.path.join(path, file))\n    us = raw[raw[\"Country_Region\"] == \"US\"]\n    return us",
    "id": "8ee86240c8104a5d847193571e44b71d",
    "idx": 42,
    "time": "2021-01-27T00:56:59.012Z",
    "type": "execution"
   },
   {
    "id": "8ee86240c8104a5d847193571e44b71d",
    "time": "2021-01-27T00:56:59.076Z",
    "type": "completion"
   },
   {
    "code": "covid_US = covidUsDf().loc[:, [\"FIPS\", \"Incident_Rate\"]]\ncovid_US[\"FIPS\"] = fipsIntToString(covid_US[\"FIPS\"])\ncovid_US.head()",
    "id": "b578d0378b9749518fc8badf9e49cf9b",
    "idx": 44,
    "time": "2021-01-27T00:57:00.970Z",
    "type": "execution"
   },
   {
    "id": "b578d0378b9749518fc8badf9e49cf9b",
    "time": "2021-01-27T00:57:01.225Z",
    "type": "completion"
   },
   {
    "code": "def downloadCOVID(out = raw_path):\n    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/12-31-2020.csv\"\n    \n    file = url.split(\"/\")[-1]\n\n    if os.path.exists(out):\n        if os.path.exists(os.path.join(out, file)):\n            print(file, \"already exists\")\n        else:\n          wget.download(url=url, out=out)\n    else:\n        print(f\"The directory {out} does not exist\")\n    ",
    "id": "1f7b3882ef354969b593ed11f9e8cd50",
    "idx": 40,
    "time": "2021-01-27T01:00:43.862Z",
    "type": "execution"
   },
   {
    "id": "1f7b3882ef354969b593ed11f9e8cd50",
    "time": "2021-01-27T01:00:43.928Z",
    "type": "completion"
   },
   {
    "code": "downloadCOVID()",
    "id": "750a1d51134a4bd08f3c3078040cee72",
    "idx": 41,
    "time": "2021-01-27T01:00:47.752Z",
    "type": "execution"
   },
   {
    "id": "750a1d51134a4bd08f3c3078040cee72",
    "time": "2021-01-27T01:00:47.979Z",
    "type": "completion"
   },
   {
    "code": "def covidUsDf(path = raw_path, file = \"12-31-2020.csv\"):\n    \"\"\"\n    Takes a path and a file containing data from the CSSE COVID-19 Daily Reports\n\n    Returns a dataframe with only US data\n    \"\"\"\n    raw = pd.read_csv(os.path.join(path, file))\n    us = raw[raw[\"Country_Region\"] == \"US\"]\n    return us",
    "id": "8ee86240c8104a5d847193571e44b71d",
    "idx": 42,
    "time": "2021-01-27T01:01:21.007Z",
    "type": "execution"
   },
   {
    "id": "8ee86240c8104a5d847193571e44b71d",
    "time": "2021-01-27T01:01:21.069Z",
    "type": "completion"
   },
   {
    "code": "covid_US = covidUsDf().loc[:, [\"FIPS\", \"Incident_Rate\"]]\ncovid_US[\"FIPS\"] = fipsIntToString(covid_US[\"FIPS\"])\ncovid_US.head()",
    "id": "b578d0378b9749518fc8badf9e49cf9b",
    "idx": 44,
    "time": "2021-01-27T01:01:22.988Z",
    "type": "execution"
   },
   {
    "id": "b578d0378b9749518fc8badf9e49cf9b",
    "time": "2021-01-27T01:01:23.156Z",
    "type": "completion"
   },
   {
    "code": "census_weather_covid_merge = census_fips.merge(weather_FIPS_agg, how= \"inner\", on = \"FIPS\").merge(covid_US, how= \"inner\", on = \"FIPS\")\ncensus_weather_covid_merge",
    "id": "b6f3745e6b5f4d1687de35b9ea63673a",
    "idx": 46,
    "time": "2021-01-27T01:02:13.630Z",
    "type": "execution"
   },
   {
    "id": "b6f3745e6b5f4d1687de35b9ea63673a",
    "time": "2021-01-27T01:02:13.808Z",
    "type": "completion"
   },
   {
    "code": "def fipsGetAll():\n    \"\"\"\n    Downloads and returns all unique US FIPs codes from kjhealy's github repo as integers.\n    \"\"\"\n    df = pd.read_csv(\"https://raw.githubusercontent.com/kjhealy/fips-codes/master/county_fips_master.csv\", \n                     encoding = 'cp1252')\n    fips = df.loc[:,[\"fips\"]].rename(columns = {\"fips\": \"FIPS\"})\n    return fips",
    "id": "765033cb35f147a19043d32976188fb0",
    "idx": 48,
    "time": "2021-01-27T01:02:30.578Z",
    "type": "execution"
   },
   {
    "id": "765033cb35f147a19043d32976188fb0",
    "time": "2021-01-27T01:02:30.638Z",
    "type": "completion"
   },
   {
    "code": "all_fips = fipsGetAll()\nall_fips[\"FIPS\"] = fipsIntToString(all_fips[\"FIPS\"])",
    "id": "d6a49cfc0ec644798b793c9ab20a39ac",
    "idx": 49,
    "time": "2021-01-27T01:02:49.010Z",
    "type": "execution"
   },
   {
    "id": "d6a49cfc0ec644798b793c9ab20a39ac",
    "time": "2021-01-27T01:02:49.464Z",
    "type": "completion"
   },
   {
    "code": "def fipsCoverage(df, allfips):\n    \"\"\"\n    Takes a dataframe with some FIPS codes and compares it to another dataframe with all FIPS codes, both of which must have a \"FIPS\" columns.\n    Returns the percent of FIPS codes covered in the first dataframe.\n    \"\"\"\n    proportion = int(df.merge(allfips, on = \"FIPS\").shape[0]/ allfips.shape[0]*100)\n    return proportion",
    "id": "8af1a78ab73e4dca83a23eb60f3eb8cd",
    "idx": 50,
    "time": "2021-01-27T01:02:53.235Z",
    "type": "execution"
   },
   {
    "id": "8af1a78ab73e4dca83a23eb60f3eb8cd",
    "time": "2021-01-27T01:02:53.318Z",
    "type": "completion"
   },
   {
    "code": "print(f\"There are {census_fips.shape[0]} counties in the Census dataset with {fipsCoverage(census_fips, all_fips)}% coverage.\")\nprint(f\"There are {weather_FIPS_agg.shape[0]} counties in the Climate dataset with {fipsCoverage(weather_FIPS_agg, all_fips)}% coverage.\")\nprint(f\"There are {covid_US.shape[0]} counties in the COVID-19 dataset with {fipsCoverage(covid_US, all_fips)}% coverage.\")\nprint(f\"There are {census_weather_covid_merge.shape[0]} counties in the merged dataset with {fipsCoverage(census_weather_covid_merge, all_fips)}% coverage.\")",
    "id": "16be2645779d40d4aa23c3bdec4854cd",
    "idx": 51,
    "time": "2021-01-27T01:03:01.653Z",
    "type": "execution"
   },
   {
    "id": "16be2645779d40d4aa23c3bdec4854cd",
    "time": "2021-01-27T01:03:01.748Z",
    "type": "completion"
   },
   {
    "code": "# Map of U.S. Counties\ndef mapCounties(df, title = \"US Counties\", savefig = False, fig_dir =\"Figures\"):\n\n    \"\"\"\n    Takes a dataframe with a \"FIPS\" column.\n    Returns a map of the US with the present counties highlighted.\n    \"\"\"    \n\n    with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n        counties = json.load(response)\n\n    fig = px.choropleth(df, \n                        geojson=counties, \n                        locations='FIPS',\n                        scope=\"usa\")\n\n    fig.update_layout(title = title,\n                      showlegend = False)\n    if savefig:\n        if \"html\" in savefig:\n            return plotly.offline.plot(fig, filename= os.path.join(fig_dir, savefig))\n        else:\n            return fig.write_image(os.path.join(fig_dir, savefig))\n    else:\n        return fig",
    "id": "ca6c82fb79f3414886e39f195847f26b",
    "idx": 53,
    "time": "2021-01-27T01:06:00.914Z",
    "type": "execution"
   },
   {
    "id": "ca6c82fb79f3414886e39f195847f26b",
    "time": "2021-01-27T01:06:00.980Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(census_fips, \ntitle = f'Counties in United States 2019 Census Estimates ({fipsCoverage(census_fips, all_fips)}% coverage)',\nsavefig= \"Counties_2019_ACS1_Census.html\")",
    "id": "d2757d0ec933407883104cd9b8d62830",
    "idx": 54,
    "time": "2021-01-27T01:06:07.570Z",
    "type": "execution"
   },
   {
    "id": "d2757d0ec933407883104cd9b8d62830",
    "time": "2021-01-27T01:06:12.935Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(weather_FIPS_agg, \ntitle= f\"Counties in GHCN weather dataset ({fipsCoverage(weather_FIPS_agg, all_fips)}% coverage)\",\nsavefig= \"Counties_2019_ACS1_Weather.html\")",
    "id": "c88892c4a288480684e297ced737a6db",
    "idx": 55,
    "time": "2021-01-27T01:06:17.405Z",
    "type": "execution"
   },
   {
    "id": "c88892c4a288480684e297ced737a6db",
    "time": "2021-01-27T01:06:22.577Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(covid_US, \ntitle= f\"Counties in COVID-19 dataset ({fipsCoverage(covid_US, all_fips)}% coverage)\",\nsavefig= \"Counties_2019_ACS1_COVID19.html\")",
    "id": "712c5ea73159425aafb918abd0b63bcd",
    "idx": 56,
    "time": "2021-01-27T01:06:23.703Z",
    "type": "execution"
   },
   {
    "id": "712c5ea73159425aafb918abd0b63bcd",
    "time": "2021-01-27T01:06:29.034Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(census_weather_covid_merge, \ntitle= f\"Counties in Merged Census, Weather, and COVID Dataset({fipsCoverage(census_weather_covid_merge, all_fips)}% coverage)\", \nsavefig= \"Counties_2019_ACS1_Census_Weather_COVID.html\")",
    "id": "09d5efeb43084c4bb3eaedcf40dc026d",
    "idx": 57,
    "time": "2021-01-27T01:06:30.091Z",
    "type": "execution"
   },
   {
    "id": "09d5efeb43084c4bb3eaedcf40dc026d",
    "time": "2021-01-27T01:06:35.408Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPS_agg.shape()",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:07:34.018Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:07:34.190Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPS_agg.shape",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:07:37.167Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:07:37.249Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\n Records: {leg(weather_FIPS_agg)}\\n, Columns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:10:21.306Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:10:21.412Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\n Records: {len(weather_FIPS_agg)}\\n, Columns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:10:32.418Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:10:32.522Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\n Records: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:10:39.035Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:10:39.120Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\nRecords: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:10:43.332Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:10:43.411Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\nNumber of Records: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:10:53.571Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:10:53.659Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\nNumber of Counties: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:11:01.490Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:11:01.575Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\nNumber of Counties: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "fb99d05c89d54420baf033837d2759c2",
    "idx": 38,
    "time": "2021-01-27T01:11:13.415Z",
    "type": "execution"
   },
   {
    "id": "fb99d05c89d54420baf033837d2759c2",
    "time": "2021-01-27T01:11:13.493Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPs_agg",
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "idx": 39,
    "time": "2021-01-27T01:11:32.902Z",
    "type": "execution"
   },
   {
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "time": "2021-01-27T01:11:32.971Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPS_agg",
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "idx": 39,
    "time": "2021-01-27T01:11:39.090Z",
    "type": "execution"
   },
   {
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "time": "2021-01-27T01:11:39.200Z",
    "type": "completion"
   },
   {
    "code": "weather_FIPS_agg[1:150]",
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "idx": 39,
    "time": "2021-01-27T01:11:52.059Z",
    "type": "execution"
   },
   {
    "id": "18fe2cec6d77468a8bedfdefbab865e2",
    "time": "2021-01-27T01:11:52.321Z",
    "type": "completion"
   },
   {
    "code": "# Use the same features extracted for the acs1 census estimates\n\nacs5_census= censusApiToDf(year_estimate=\"acs5\", features= census_features_all).rename(columns=census_colnames)\nacs5_census[\"FIPS\"] = acs5_census[\"state\"] + acs5_census[\"county\"]\nacs5_census.drop(columns = [\"state\", \"county\"], inplace= True)",
    "id": "e52c90877b2e427c89d89cb395a7ad7a",
    "idx": 61,
    "time": "2021-01-27T01:12:59.482Z",
    "type": "execution"
   },
   {
    "code": "acs5_covid_merge = acs5_census.merge(covid_US, how= \"inner\", on = \"FIPS\")",
    "id": "51e8bdaf7c8940868026f22f8bc705ff",
    "idx": 62,
    "time": "2021-01-27T01:13:00.921Z",
    "type": "execution"
   },
   {
    "id": "e52c90877b2e427c89d89cb395a7ad7a",
    "time": "2021-01-27T01:13:01.117Z",
    "type": "completion"
   },
   {
    "id": "51e8bdaf7c8940868026f22f8bc705ff",
    "time": "2021-01-27T01:13:01.128Z",
    "type": "completion"
   },
   {
    "code": "acs5_covid_merge",
    "id": "7a281d9bd9514e9385b814618afca71b",
    "idx": 63,
    "time": "2021-01-27T01:13:01.851Z",
    "type": "execution"
   },
   {
    "id": "7a281d9bd9514e9385b814618afca71b",
    "time": "2021-01-27T01:13:02.033Z",
    "type": "completion"
   },
   {
    "code": "print(f\"There are {acs5_census.shape[0]} counties in the ACS 5-year Census dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")\nprint(f\"There are {acs5_covid_merge.shape[0]} counties in the merged prediction dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")",
    "id": "36207a2540ac493bbc50b69da8a6af17",
    "idx": 64,
    "time": "2021-01-27T01:13:05.082Z",
    "type": "execution"
   },
   {
    "id": "36207a2540ac493bbc50b69da8a6af17",
    "time": "2021-01-27T01:13:05.155Z",
    "type": "completion"
   },
   {
    "code": "def write_csv(df, path, filename):\n    \"\"\"\n    Takes a dataframe, path, and filename.\n    Returns a written csv.\n    Checks if the csv is already written, and if the path is correct.\n    \"\"\"\n    file = os.path.join(path, filename)\n\n    if os.path.exists(path):\n        if os.path.exists(file):\n            print(f\"The csv {filename} already exists.\")\n        else:\n            pd.DataFrame.to_csv(df, file)\n    else:\n        print(f\"The path {path} does not exist.\")",
    "id": "5a142cf1db3f4c2787da2fc2547ecbf4",
    "idx": 66,
    "time": "2021-01-27T01:13:24.457Z",
    "type": "execution"
   },
   {
    "id": "5a142cf1db3f4c2787da2fc2547ecbf4",
    "time": "2021-01-27T01:13:24.512Z",
    "type": "completion"
   },
   {
    "code": "def write_csv(df, path, filename):\n    \"\"\"\n    Takes a dataframe, path, and filename.\n    Returns a written csv.\n    Checks if the csv is already written, and if the path is correct.\n    \"\"\"\n    file = os.path.join(path, filename)\n\n    if os.path.exists(path):\n        if os.path.exists(file):\n            print(f\"The csv {filename} already exists.\")\n        else:\n            pd.DataFrame.to_csv(df, file)\n    else:\n        print(f\"The path {path} does not exist.\")",
    "id": "5a142cf1db3f4c2787da2fc2547ecbf4",
    "idx": 66,
    "time": "2021-01-27T01:15:15.452Z",
    "type": "execution"
   },
   {
    "id": "5a142cf1db3f4c2787da2fc2547ecbf4",
    "time": "2021-01-27T01:15:15.509Z",
    "type": "completion"
   },
   {
    "code": "census_weather_covid_inference_name = \"census_weather_covid_inference.csv\"\n\nwrite_csv(census_weather_covid_merge, clean_path, census_weather_covid_inference_name)",
    "id": "6d0e3296119345668584662e34079c8d",
    "idx": 67,
    "time": "2021-01-27T01:15:22.178Z",
    "type": "execution"
   },
   {
    "id": "6d0e3296119345668584662e34079c8d",
    "time": "2021-01-27T01:15:22.246Z",
    "type": "completion"
   },
   {
    "code": "census_covid_prediction_name = \"census_covid_prediction.csv\"\nwrite_csv(acs5_covid_merge, clean_path, census_covid_prediction_name)",
    "id": "91120976c49747668f05ef15fd8c5432",
    "idx": 68,
    "time": "2021-01-27T01:15:24.293Z",
    "type": "execution"
   },
   {
    "id": "91120976c49747668f05ef15fd8c5432",
    "time": "2021-01-27T01:15:24.396Z",
    "type": "completion"
   },
   {
    "code": "# A few packages you may need, uncomment to download via pip !\n#!pip install reverse_geocoder\n#!pip install wget\n#!pip install reverse_geocoder\n#!pip install kaleido",
    "id": "16adc19c120b479c9da4fe5ab7378364",
    "idx": 0,
    "time": "2021-01-27T01:19:12.717Z",
    "type": "execution"
   },
   {
    "id": "16adc19c120b479c9da4fe5ab7378364",
    "time": "2021-01-27T01:19:12.828Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "1972bc0421f34e7b9b6abef6f2b9049c",
    "idx": 1,
    "time": "2021-01-29T07:09:06.914Z",
    "type": "execution"
   },
   {
    "id": "1972bc0421f34e7b9b6abef6f2b9049c",
    "time": "2021-01-29T07:09:07.629Z",
    "type": "completion"
   },
   {
    "code": "# A few packages you may need, uncomment to download via pip !\n!pip install reverse_geocoder\n!pip install wget\n!pip install reverse_geocoder\n!pip install kaleido",
    "id": "dbae9bfd1d90443db88afee1994c8f80",
    "idx": 0,
    "time": "2021-01-29T07:09:20.212Z",
    "type": "execution"
   },
   {
    "id": "dbae9bfd1d90443db88afee1994c8f80",
    "time": "2021-01-29T07:09:38.389Z",
    "type": "completion"
   },
   {
    "code": "# A few packages you may need, uncomment to download via pip !\n#!pip install reverse_geocoder\n#!pip install wget\n#!pip install reverse_geocoder\n#!pip install kaleido",
    "id": "dbae9bfd1d90443db88afee1994c8f80",
    "idx": 0,
    "time": "2021-01-29T07:09:55.443Z",
    "type": "execution"
   },
   {
    "id": "dbae9bfd1d90443db88afee1994c8f80",
    "time": "2021-01-29T07:09:55.554Z",
    "type": "completion"
   },
   {
    "code": "#import required packages\nimport pandas as pd\nimport numpy as np\nimport urllib\nimport requests\nimport urllib\nimport wget\nimport os\nimport re\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import Image\nfrom urllib.request import urlopen\nimport json\nimport reverse_geocoder as rg",
    "id": "1972bc0421f34e7b9b6abef6f2b9049c",
    "idx": 1,
    "time": "2021-01-29T07:09:56.249Z",
    "type": "execution"
   },
   {
    "id": "1972bc0421f34e7b9b6abef6f2b9049c",
    "time": "2021-01-29T07:09:57.235Z",
    "type": "completion"
   },
   {
    "code": "pd.set_option('display.max_rows', 200)\npd.set_option('display.max_colwidth', -1)",
    "id": "3f27a424889b4576af1f20141fd21fde",
    "idx": 2,
    "time": "2021-01-29T07:09:59.185Z",
    "type": "execution"
   },
   {
    "id": "3f27a424889b4576af1f20141fd21fde",
    "time": "2021-01-29T07:09:59.266Z",
    "type": "completion"
   },
   {
    "code": "raw_path = os.path.join(\"data\", \"raw\")\nclean_path = os.path.join(\"data\", \"clean\")",
    "id": "988ccbcafc78435985b16efe503c4896",
    "idx": 3,
    "time": "2021-01-29T07:09:59.723Z",
    "type": "execution"
   },
   {
    "id": "988ccbcafc78435985b16efe503c4896",
    "time": "2021-01-29T07:09:59.791Z",
    "type": "completion"
   },
   {
    "code": "def makeDataDir():\n    \"\"\"\n    Creates a data directory with clean and raw subdirectories.\n    \"\"\"\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n        print(\"created data directory\")\n        \n        if not os.path.exists(raw_path):\n            os.makedirs(raw_path)\n            print(f\"created {raw_path} directory\")\n        else:\n            print(f\"{raw_path} directory exists!\")\n        \n        if not os.path.exists(clean_path):\n            os.makedirs(clean_path)\n            print(f\"created {clean_path} directory\")     \n        else:\n            print(f\"{clean_path} directory exists!\")\n    \n    else:\n        print(\"data directory exists!\")",
    "id": "05ad479968de4fed83fe605bebbf8a9c",
    "idx": 4,
    "time": "2021-01-29T07:10:01.149Z",
    "type": "execution"
   },
   {
    "id": "05ad479968de4fed83fe605bebbf8a9c",
    "time": "2021-01-29T07:10:01.215Z",
    "type": "completion"
   },
   {
    "code": "makeDataDir()",
    "id": "c8141da1b0114efcae1d7caba8b82119",
    "idx": 5,
    "time": "2021-01-29T07:10:01.814Z",
    "type": "execution"
   },
   {
    "id": "c8141da1b0114efcae1d7caba8b82119",
    "time": "2021-01-29T07:10:01.909Z",
    "type": "completion"
   },
   {
    "code": "def censusApiToDf(year_estimate = \"acs5\", features = False, metadata_only = False, geography = \"county:*&in=state:*\"):\n    \"\"\"\n    Takes parameters to extract data for the ACS 2019 DP03 and DP05 datasets.\n    Returns a dataframe of ACS features or variables at the county level.\n    \n    Allows the following parameters to personalize query:\n    \n    1. year_estimate - Choose between acs1 and acs5 for 1-year and 5-year estimates.\n        1-year estimates has more current, but less data.\n        5-year estimates has less current, but more data.\n        \n    2. features - A list of features to extract from ACS data.\n    \n    3. metadata_only - If True, returns the metadata of features for a given datatable.\n    \n    4. geography - Specify US locations to extract. Defaults to extracting all counties.\n    \"\"\"\n    \n    baseAPI = f\"https://api.census.gov/data/2019/acs/{year_estimate}\"\n    \n    \n    if metadata_only:\n        url = f\"{baseAPI}/profile/variables\"\n        \n    elif features:\n        features = \",\".join(features)\n        url = f\"{baseAPI}/profile?get=NAME,{features}&for={geography}\"\n        \n    else:\n        return print(\"Error: Must either set variables = True, or pass input features to extract.\")\n\n    response = requests.get(url)\n    jsonResponse = json.loads(response.text)\n    \n    raw = pd.DataFrame(data= jsonResponse)\n    headers = raw.iloc[0]\n    raw.columns = headers\n    df = raw.loc[1:,:]\n    \n    return df",
    "id": "4e8e3e2e038441b3b3b16bda1718b79c",
    "idx": 7,
    "time": "2021-01-29T07:10:04.193Z",
    "type": "execution"
   },
   {
    "id": "4e8e3e2e038441b3b3b16bda1718b79c",
    "time": "2021-01-29T07:10:04.258Z",
    "type": "completion"
   },
   {
    "code": "census_metadata = censusApiToDf(metadata_only = True).loc[4:, [\"name\", \"label\"]]",
    "id": "df6c51a92dc24bc69513446775933982",
    "idx": 8,
    "time": "2021-01-29T07:10:05.159Z",
    "type": "execution"
   },
   {
    "code": "census_metadata.sample(frac=1, random_state=8).head()",
    "id": "51e13d136e4b41228550d6582dc16622",
    "idx": 9,
    "time": "2021-01-29T07:10:05.582Z",
    "type": "execution"
   },
   {
    "id": "df6c51a92dc24bc69513446775933982",
    "time": "2021-01-29T07:10:05.659Z",
    "type": "completion"
   },
   {
    "id": "51e13d136e4b41228550d6582dc16622",
    "time": "2021-01-29T07:10:05.676Z",
    "type": "completion"
   },
   {
    "code": "def splitCensusMetadata(df):\n    \"\"\"\n    Takes the raw census metdata dataframe and splits the name column by the \"!!\" delimiter\n    \"\"\"\n    split = df.join(df['label'].str.split(\"!!\", expand=True))\n    return split",
    "id": "7c98fc3abb194a35a1a7263ade48b24a",
    "idx": 11,
    "time": "2021-01-29T07:10:07.829Z",
    "type": "execution"
   },
   {
    "id": "7c98fc3abb194a35a1a7263ade48b24a",
    "time": "2021-01-29T07:10:07.886Z",
    "type": "completion"
   },
   {
    "code": "splitCensusMetadata(census_metadata).sample(frac=1, random_state=8).head()",
    "id": "85ceff410e4d47778e51eff33020bca3",
    "idx": 12,
    "time": "2021-01-29T07:10:08.223Z",
    "type": "execution"
   },
   {
    "id": "85ceff410e4d47778e51eff33020bca3",
    "time": "2021-01-29T07:10:08.313Z",
    "type": "completion"
   },
   {
    "code": "def findPatterns(df, col, patterns):\n    \"\"\"\n    Takes a dataframe, specified columns, and a list of string patterns.\n    Returns a dataframe with matches to all strings patterns.\n    \"\"\"\n\n    string_masks = (df[col].str.contains(string, regex = True, case = False) for string in patterns)\n    comb_mask = np.vstack(string_masks).all(axis=0)\n    final_df = df[comb_mask]\n\n    return final_df",
    "id": "b097a933a04549ad8b29500d4e91149f",
    "idx": 14,
    "time": "2021-01-29T07:10:08.968Z",
    "type": "execution"
   },
   {
    "id": "b097a933a04549ad8b29500d4e91149f",
    "time": "2021-01-29T07:10:09.024Z",
    "type": "completion"
   },
   {
    "code": "race_ethnicity_cols = findPatterns(df = census_metadata,\n                    col = \"label\",\n                    patterns= [\"percent!!\", \n                    \"total\", \n                    \"one race|hispanic or latino \\(of any race\\)$\", \n                    \"black|white|pacific islander$|asian$|hispanic\"])\n\nfiltered_ethnicity_cols = race_ethnicity_cols[~race_ethnicity_cols[\"label\"].str.contains(\"Other Asian|!!Other Pacific Islander\")]\nDP05_cols = filtered_ethnicity_cols[\"name\"].values\nfiltered_ethnicity_cols",
    "id": "a3160245eef8467299fe1688bfc4e00b",
    "idx": 15,
    "time": "2021-01-29T07:10:09.210Z",
    "type": "execution"
   },
   {
    "id": "a3160245eef8467299fe1688bfc4e00b",
    "time": "2021-01-29T07:10:09.309Z",
    "type": "completion"
   },
   {
    "code": "income_pat = [\"estimate!!\", \"income\", \"median household\"]\ninsurance_pat = [\"percent!!\", \"with health insurance coverage$\", \"noninstitutionalized population!!\"]\npoverty_pat = [\"percent!!\", \"below the poverty level\", \"all families$\"]\n\neconomic_cols = pd.concat([findPatterns(census_metadata, \"label\", income_pat),\n                          findPatterns(census_metadata, \"label\", insurance_pat),\n                          findPatterns(census_metadata, \"label\", poverty_pat)])\n\nDP03_cols = economic_cols[\"name\"].values\neconomic_cols",
    "id": "1d591cf38593494782de0648633f0408",
    "idx": 17,
    "time": "2021-01-29T07:10:09.787Z",
    "type": "execution"
   },
   {
    "id": "1d591cf38593494782de0648633f0408",
    "time": "2021-01-29T07:10:09.891Z",
    "type": "completion"
   },
   {
    "code": "census_features_all = list(DP03_cols) + list(DP05_cols)\ncensus_colnames = {\"DP05_0037PE\": \"white_perc\",\n                    \"DP05_0038PE\": \"black_perc\",\n                    \"DP05_0039PE\": \"amer_native_perc\",\n                    \"DP05_0044PE\": \"asian_perc\",\n                    \"DP05_0052PE\": \"pacific_perc\",\n                    \"DP05_0071PE\": \"hispanic_perc\",\n                    \"DP03_0062E\": \"income_med_dollars\",\n                    \"DP03_0096PE\": \"insurance_perc\",\n                    \"DP03_0119PE\": \"poverty_perc\"}\n\ncensus_fips = censusApiToDf(year_estimate=\"acs1\", features= census_features_all).rename(columns=census_colnames)\ncensus_fips[\"FIPS\"] = census_fips[\"state\"] + census_fips[\"county\"]\ncensus_fips.drop(columns = [\"state\", \"county\"], inplace= True)\n\ncensus_fips",
    "id": "4f1ff3b8b1a94b66aea424eea8dd91bb",
    "idx": 19,
    "time": "2021-01-29T07:10:10.275Z",
    "type": "execution"
   },
   {
    "code": "def downloadStations(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"ghcnd-stations.txt\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n        file = wget.download(url, out = out)\n    else:\n        print(\"ghcnd-stations.txt already exists\")\n\ndef downloadWeather(out = raw_path):\n    if not os.path.exists(os.path.join(out, \"2020.csv\")):\n        url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz'\n        wget.download(url, out = out)\n\n        !gzip -d data/raw/2020.csv.gz\n    else:\n        print(\"2020.csv already exists\")",
    "id": "38b94db0983d49bf86ab84b82b02ae4a",
    "idx": 21,
    "time": "2021-01-29T07:10:11.546Z",
    "type": "execution"
   },
   {
    "id": "4f1ff3b8b1a94b66aea424eea8dd91bb",
    "time": "2021-01-29T07:10:11.896Z",
    "type": "completion"
   },
   {
    "id": "38b94db0983d49bf86ab84b82b02ae4a",
    "time": "2021-01-29T07:10:11.906Z",
    "type": "completion"
   },
   {
    "code": "def get_vals(line):\n    ls = line.split(',')\n    station = ls[0]\n    time = ls[1]\n    val = float(ls[3])\n    return [station, time, val]\n\ndef get_stations(filename = os.path.join(raw_path, 'ghcnd-stations.txt')):\n    df = pd.read_csv(filename, '/t', header=None)\n    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n    df_drop = df[df[\"Station\"].str.startswith(\"US\")].reset_index(drop = True)\n    return df_drop\n\ndef process_year(col='TAVG', path = raw_path):\n    tavg = []\n    pattern = re.compile(r\"^US.*TAVG\")\n    with open(os.path.join(path, \"2020.csv\")) as h:\n        l = h.readline()\n        while l:\n            if re.match(pattern, l) is not None:\n                v = get_vals(l)\n                tavg.append(get_vals(l))\n            l = h.readline()\n    df_tavg = pd.DataFrame(tavg, columns=['Station', 'Date', col])\n    return df_tavg\n\ndef mergeStationAggYear(stations, df_tavg):\n    merged = df_tavg.merge(stations, on='Station', how='inner')\n    \n    merged[\"TAVG\"] = merged[\"TAVG\"]/10 + 273\n\n    # Get mean temp and temp standard deviation across year\n    agg_df = merged.loc[:,[\"Station\",\"Latitude\", \"Longitude\", \"TAVG\"]].groupby([\"Station\", \"Latitude\", \"Longitude\"]).agg(\n        temp_mean = (\"TAVG\", np.mean),\n        temp_std = (\"TAVG\", np.std))\n\n    #Convert temperture mean to celsius for intepretability\n    agg_df[\"temp_mean\"] = agg_df[\"temp_mean\"] - 273\n    return agg_df",
    "id": "fc2a539d2cf44a43861ecfb1b61e653c",
    "idx": 22,
    "time": "2021-01-29T07:10:12.178Z",
    "type": "execution"
   },
   {
    "id": "fc2a539d2cf44a43861ecfb1b61e653c",
    "time": "2021-01-29T07:10:12.245Z",
    "type": "completion"
   },
   {
    "code": "def weatherToDf():\n    \"\"\"\n    The full pipeline to:\n    1. Download weather station temperature data (US only)\n    2. Merge station location to temp data\n    3. Aggregate mean temperature, and standard deviation across the year\n    \"\"\"\n    downloadStations()\n    print(\"Downloaded weather stations\")\n    downloadWeather()\n    print(\"Downloaded temperature data.\")\n    \n    stations = get_stations()\n    df_tavg = process_year(col='TAVG')\n    merged_agg = mergeStationAggYear(stations, df_tavg).reset_index()\n    \n    return merged_agg",
    "id": "331fd155111e4567850f6cd765f3cd26",
    "idx": 23,
    "time": "2021-01-29T07:10:13.102Z",
    "type": "execution"
   },
   {
    "id": "331fd155111e4567850f6cd765f3cd26",
    "time": "2021-01-29T07:10:13.186Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg = weatherToDf()",
    "id": "df4d1c95784447d180636687a7f66ace",
    "idx": 24,
    "time": "2021-01-29T07:10:14.436Z",
    "type": "execution"
   },
   {
    "id": "df4d1c95784447d180636687a7f66ace",
    "time": "2021-01-29T07:11:03.406Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.head()",
    "id": "4c27ec57f6e64c27b92381f48108ab3e",
    "idx": 25,
    "time": "2021-01-29T07:14:58.561Z",
    "type": "execution"
   },
   {
    "id": "4c27ec57f6e64c27b92381f48108ab3e",
    "time": "2021-01-29T07:14:58.671Z",
    "type": "completion"
   },
   {
    "code": "weather_station_agg.describe()",
    "id": "5206d9775a7b4b0ababe5d3217babcb0",
    "idx": 26,
    "time": "2021-01-29T07:14:59.144Z",
    "type": "execution"
   },
   {
    "id": "5206d9775a7b4b0ababe5d3217babcb0",
    "time": "2021-01-29T07:14:59.226Z",
    "type": "completion"
   },
   {
    "code": "#Reverse geocode latitutude and longitutes to county and state\n#Function to reverse_geocode counties\n\ndef addCountyStateFromLatLong(dataframe, lat, long, US_only = False):\n    \"\"\"\n    Takes a dataframe with latitude and longitude and adds the county and state.\n    \n    Inputs:\n    1) Dataframe \n    2) Latitude Column\n    3) Longitude Column\n    \n    Returns: \n    1) Dataframe with County and State\n    \"\"\"\n    \n    #Use lat and long as tuples to extract county and state \n    query = rg.search([tuple(x) for x in dataframe[[lat, long]].values])\n    \n    #Initialize lists to extract county and state from our query\n    state = []\n    county = []\n    country = []\n    \n    if US_only:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n            country.append(query[i][\"cc\"])\n    else:\n        for i in np.arange(0,len(query)):\n            state.append(query[i][\"admin1\"])\n            county.append(query[i][\"admin2\"])\n    \n    #Create dataframe of filled lists\n    if US_only:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county,\n                        \"country\": country})\n    else:\n        sc_df = pd.DataFrame({\"state\": state,\n                        \"county\": county})\n    \n    #assert that the query and station dataframes are the same size\n    assert len(dataframe) == len(sc_df), \"Row lengths don't match for input and output. Check lat/long values in input.\"\n    \n    #Merge the Station with county and state horizontally\n    if US_only:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n\n        concat_df = concat_df[concat_df[\"country\"] == \"US\"].drop(columns = \"country\")\n    else:\n        concat_df = pd.concat([dataframe,\n                           sc_df],\n                           axis = 1)\n    \n    return concat_df",
    "id": "030a4929e7bc4e6c80a8cee7f2bcebb0",
    "idx": 28,
    "time": "2021-01-29T07:15:00.121Z",
    "type": "execution"
   },
   {
    "id": "030a4929e7bc4e6c80a8cee7f2bcebb0",
    "time": "2021-01-29T07:15:00.181Z",
    "type": "completion"
   },
   {
    "code": "weather_station_state_county = addCountyStateFromLatLong(weather_station_agg, \"Latitude\", \"Longitude\", US_only=True)",
    "id": "640dccd1a8ed4b688a1cc3534be0a291",
    "idx": 29,
    "time": "2021-01-29T07:15:00.704Z",
    "type": "execution"
   },
   {
    "code": "weather_station_state_county.head()",
    "id": "8c4f16b52ee749c59156e077d3618804",
    "idx": 30,
    "time": "2021-01-29T07:15:01.436Z",
    "type": "execution"
   },
   {
    "id": "640dccd1a8ed4b688a1cc3534be0a291",
    "time": "2021-01-29T07:15:01.974Z",
    "type": "completion"
   },
   {
    "id": "8c4f16b52ee749c59156e077d3618804",
    "time": "2021-01-29T07:15:01.983Z",
    "type": "completion"
   },
   {
    "code": "def fipsIntToString(sr):\n    \"\"\"\n    Takes a series of FIPS integers with NA values\n    Returns a FIPS string 5 with 0 padding up front\n    \"\"\"\n    string = sr.fillna(0).astype(\"int32\").astype(\"str\")\n    padding = string.apply(lambda x: \"0\"*(5 - len(x)) + x)\n    return padding",
    "id": "5bec8f181013499d8ec628653203b816",
    "idx": 31,
    "time": "2021-01-29T07:15:02.696Z",
    "type": "execution"
   },
   {
    "id": "5bec8f181013499d8ec628653203b816",
    "time": "2021-01-29T07:15:02.759Z",
    "type": "completion"
   },
   {
    "code": "#Helper function to join county and state with FIPS from a lookup table\ndef add_FIPS(dataframe):\n    \"\"\"\n    This function joins FIPS codes to the state and county outputs of reverse_geocoder\n    Inputs:\n    1. Dataframe with \"state\" and \"county\" columns\n    2. Path to FIPS lookup table\n    \"\"\"\n    #Download FIPS lookuptable with relevant columns\n    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\"\n    lookuptable= pd.read_csv(url).loc[:,[\"FIPS\", \"Combined_Key\"]]\n    \n    #Add the FIPS codes\n    add_FIPS = dataframe.copy()\n    add_FIPS[\"Combined_Key\"] = add_FIPS[\"county\"].str.replace(\" County\", \", \") + add_FIPS[\"state\"] + \", US\"\n    climate_FIPS = add_FIPS.merge(lookuptable, on = \"Combined_Key\")\n    climate_FIPS.drop(columns = [\"state\", \"county\", \"Combined_Key\"], inplace = True)\n\n    #Convert FIPS to 5 digit string\n    climate_FIPS[\"FIPS\"] = fipsIntToString(climate_FIPS[\"FIPS\"])\n    return climate_FIPS",
    "id": "53e1b4812ef64954afa290fd8165c422",
    "idx": 32,
    "time": "2021-01-29T07:15:03.123Z",
    "type": "execution"
   },
   {
    "id": "53e1b4812ef64954afa290fd8165c422",
    "time": "2021-01-29T07:15:03.179Z",
    "type": "completion"
   },
   {
    "code": "weather_station_FIPS = add_FIPS(weather_station_state_county)\nweather_FIPS_agg =  weather_station_FIPS.groupby(\"FIPS\").agg(temp_mean = (\"temp_mean\", np.mean),\n                                                            temp_std = (\"temp_std\", np.mean)).reset_index()",
    "id": "61b8c9d56ae541308899406943919060",
    "idx": 34,
    "time": "2021-01-29T07:15:03.655Z",
    "type": "execution"
   },
   {
    "id": "61b8c9d56ae541308899406943919060",
    "time": "2021-01-29T07:15:03.916Z",
    "type": "completion"
   },
   {
    "code": "print(f\"Weather Dataset:\\nNumber of Counties: {len(weather_FIPS_agg)}\\nColumns = {weather_FIPS_agg.columns.values}\")",
    "id": "7049ee603ae84f048a03c4fc71a3a952",
    "idx": 35,
    "time": "2021-01-29T07:15:03.931Z",
    "type": "execution"
   },
   {
    "id": "7049ee603ae84f048a03c4fc71a3a952",
    "time": "2021-01-29T07:15:03.998Z",
    "type": "completion"
   },
   {
    "code": "def downloadCOVID(out = raw_path):\n    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/12-31-2020.csv\"\n    \n    file = url.split(\"/\")[-1]\n\n    if os.path.exists(out):\n        if os.path.exists(os.path.join(out, file)):\n            print(file, \"already exists\")\n        else:\n          wget.download(url=url, out=out)\n    else:\n        print(f\"The directory {out} does not exist\")\n    ",
    "id": "5fddf211301e495c9a7241d240b05359",
    "idx": 37,
    "time": "2021-01-29T07:15:04.730Z",
    "type": "execution"
   },
   {
    "id": "5fddf211301e495c9a7241d240b05359",
    "time": "2021-01-29T07:15:04.793Z",
    "type": "completion"
   },
   {
    "code": "downloadCOVID()",
    "id": "f3c3f3aa815846b19339eefab2552e0a",
    "idx": 38,
    "time": "2021-01-29T07:15:05.042Z",
    "type": "execution"
   },
   {
    "id": "f3c3f3aa815846b19339eefab2552e0a",
    "time": "2021-01-29T07:15:05.109Z",
    "type": "completion"
   },
   {
    "code": "def covidUsDf(path = raw_path, file = \"12-31-2020.csv\"):\n    \"\"\"\n    Takes a path and a file containing data from the CSSE COVID-19 Daily Reports\n\n    Returns a dataframe with only US data\n    \"\"\"\n    raw = pd.read_csv(os.path.join(path, file))\n    us = raw[raw[\"Country_Region\"] == \"US\"]\n    return us",
    "id": "dd4dc50b9b5a4821a3a22777f66c478a",
    "idx": 39,
    "time": "2021-01-29T07:15:05.476Z",
    "type": "execution"
   },
   {
    "id": "dd4dc50b9b5a4821a3a22777f66c478a",
    "time": "2021-01-29T07:15:05.537Z",
    "type": "completion"
   },
   {
    "code": "covid_US = covidUsDf().loc[:, [\"FIPS\", \"Incident_Rate\"]]\ncovid_US[\"FIPS\"] = fipsIntToString(covid_US[\"FIPS\"])\ncovid_US.head()",
    "id": "f51206cb9ec14ad89cb6118e21527235",
    "idx": 41,
    "time": "2021-01-29T07:15:06.859Z",
    "type": "execution"
   },
   {
    "id": "f51206cb9ec14ad89cb6118e21527235",
    "time": "2021-01-29T07:15:07.008Z",
    "type": "completion"
   },
   {
    "code": "census_weather_covid_merge = census_fips.merge(weather_FIPS_agg, how= \"inner\", on = \"FIPS\").merge(covid_US, how= \"inner\", on = \"FIPS\")\ncensus_weather_covid_merge",
    "id": "bcf752bd32304d2b861387749ee189e6",
    "idx": 43,
    "time": "2021-01-29T07:15:07.784Z",
    "type": "execution"
   },
   {
    "id": "bcf752bd32304d2b861387749ee189e6",
    "time": "2021-01-29T07:15:07.937Z",
    "type": "completion"
   },
   {
    "code": "def fipsGetAll():\n    \"\"\"\n    Downloads and returns all unique US FIPs codes from kjhealy's github repo as integers.\n    \"\"\"\n    df = pd.read_csv(\"https://raw.githubusercontent.com/kjhealy/fips-codes/master/county_fips_master.csv\", \n                     encoding = 'cp1252')\n    fips = df.loc[:,[\"fips\"]].rename(columns = {\"fips\": \"FIPS\"})\n    return fips",
    "id": "fe1f5387cee3459c8430599b5d178e8c",
    "idx": 45,
    "time": "2021-01-29T07:15:08.370Z",
    "type": "execution"
   },
   {
    "id": "fe1f5387cee3459c8430599b5d178e8c",
    "time": "2021-01-29T07:15:08.433Z",
    "type": "completion"
   },
   {
    "code": "all_fips = fipsGetAll()\nall_fips[\"FIPS\"] = fipsIntToString(all_fips[\"FIPS\"])",
    "id": "a499cb3ff6ea43a19a511f27372cd4a4",
    "idx": 46,
    "time": "2021-01-29T07:15:08.595Z",
    "type": "execution"
   },
   {
    "code": "def fipsCoverage(df, allfips):\n    \"\"\"\n    Takes a dataframe with some FIPS codes and compares it to another dataframe with all FIPS codes, both of which must have a \"FIPS\" columns.\n    Returns the percent of FIPS codes covered in the first dataframe.\n    \"\"\"\n    proportion = int(df.merge(allfips, on = \"FIPS\").shape[0]/ allfips.shape[0]*100)\n    return proportion",
    "id": "ee9d7a2f454e4f3a863e68dab3861929",
    "idx": 47,
    "time": "2021-01-29T07:15:08.805Z",
    "type": "execution"
   },
   {
    "id": "a499cb3ff6ea43a19a511f27372cd4a4",
    "time": "2021-01-29T07:15:08.918Z",
    "type": "completion"
   },
   {
    "id": "ee9d7a2f454e4f3a863e68dab3861929",
    "time": "2021-01-29T07:15:08.930Z",
    "type": "completion"
   },
   {
    "code": "print(f\"There are {census_fips.shape[0]} counties in the Census dataset with {fipsCoverage(census_fips, all_fips)}% coverage.\")\nprint(f\"There are {weather_FIPS_agg.shape[0]} counties in the Climate dataset with {fipsCoverage(weather_FIPS_agg, all_fips)}% coverage.\")\nprint(f\"There are {covid_US.shape[0]} counties in the COVID-19 dataset with {fipsCoverage(covid_US, all_fips)}% coverage.\")\nprint(f\"There are {census_weather_covid_merge.shape[0]} counties in the merged dataset with {fipsCoverage(census_weather_covid_merge, all_fips)}% coverage.\")",
    "id": "8123cd17c4c943e99f48af312c7780c7",
    "idx": 48,
    "time": "2021-01-29T07:15:09.822Z",
    "type": "execution"
   },
   {
    "id": "8123cd17c4c943e99f48af312c7780c7",
    "time": "2021-01-29T07:15:09.909Z",
    "type": "completion"
   },
   {
    "code": "# Map of U.S. Counties\ndef mapCounties(df, title = \"US Counties\", savefig = False, fig_dir =\"Figures\"):\n\n    \"\"\"\n    Takes a dataframe with a \"FIPS\" column.\n    Returns a map of the US with the present counties highlighted.\n    \"\"\"    \n\n    with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n        counties = json.load(response)\n\n    fig = px.choropleth(df, \n                        geojson=counties, \n                        locations='FIPS',\n                        scope=\"usa\")\n\n    fig.update_layout(title = title,\n                      showlegend = False)\n    if savefig:\n        if \"html\" in savefig:\n            return plotly.offline.plot(fig, filename= os.path.join(fig_dir, savefig))\n        else:\n            return fig.write_image(os.path.join(fig_dir, savefig))\n    else:\n        return fig",
    "id": "6b756241efeb488b837bf3e792b15e82",
    "idx": 50,
    "time": "2021-01-29T07:15:11.461Z",
    "type": "execution"
   },
   {
    "id": "6b756241efeb488b837bf3e792b15e82",
    "time": "2021-01-29T07:15:11.517Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(census_fips, \ntitle = f'Counties in United States 2019 Census Estimates ({fipsCoverage(census_fips, all_fips)}% coverage)',\nsavefig= \"Counties_2019_ACS1_Census.png\")",
    "id": "33045582dce04304b3e7208ac61a9471",
    "idx": 51,
    "time": "2021-01-29T07:15:12.776Z",
    "type": "execution"
   },
   {
    "code": "mapCounties(weather_FIPS_agg, \ntitle= f\"Counties in GHCN weather dataset ({fipsCoverage(weather_FIPS_agg, all_fips)}% coverage)\",\nsavefig= \"Counties_2019_Weather.png\")",
    "id": "a54d3ed71f864aa89655e8c27211b936",
    "idx": 52,
    "time": "2021-01-29T07:15:14.004Z",
    "type": "execution"
   },
   {
    "code": "mapCounties(covid_US, \ntitle= f\"Counties in COVID-19 dataset ({fipsCoverage(covid_US, all_fips)}% coverage)\",\nsavefig= \"Counties_2019_COVID19.png\")",
    "id": "d2dc6a1f7abc4ef9b167a74a09a058ea",
    "idx": 53,
    "time": "2021-01-29T07:15:14.709Z",
    "type": "execution"
   },
   {
    "code": "mapCounties(census_weather_covid_merge, \ntitle= f\"Counties in Merged Census, Weather, and COVID Dataset({fipsCoverage(census_weather_covid_merge, all_fips)}% coverage)\", \nsavefig= \"Counties_2019_ACS1_Census_Weather_COVID.png\")",
    "id": "401c3284407d468a8976b29b91685e7c",
    "idx": 54,
    "time": "2021-01-29T07:15:15.502Z",
    "type": "execution"
   },
   {
    "id": "33045582dce04304b3e7208ac61a9471",
    "time": "2021-01-29T07:15:19.906Z",
    "type": "completion"
   },
   {
    "id": "a54d3ed71f864aa89655e8c27211b936",
    "time": "2021-01-29T07:15:24.387Z",
    "type": "completion"
   },
   {
    "id": "d2dc6a1f7abc4ef9b167a74a09a058ea",
    "time": "2021-01-29T07:15:29.624Z",
    "type": "completion"
   },
   {
    "id": "401c3284407d468a8976b29b91685e7c",
    "time": "2021-01-29T07:15:34.010Z",
    "type": "completion"
   },
   {
    "code": "# Use the same features extracted for the acs1 census estimates\n\nacs5_census= censusApiToDf(year_estimate=\"acs5\", features= census_features_all).rename(columns=census_colnames)\nacs5_census[\"FIPS\"] = acs5_census[\"state\"] + acs5_census[\"county\"]\nacs5_census.drop(columns = [\"state\", \"county\"], inplace= True)",
    "id": "1f7821f509bd4e14b507aeec54823688",
    "idx": 58,
    "time": "2021-01-29T07:15:36.392Z",
    "type": "execution"
   },
   {
    "id": "1f7821f509bd4e14b507aeec54823688",
    "time": "2021-01-29T07:15:37.920Z",
    "type": "completion"
   },
   {
    "code": "acs5_covid_merge = acs5_census.merge(covid_US, how= \"inner\", on = \"FIPS\")",
    "id": "ba3411b27d204a3691c81b7c45845b6b",
    "idx": 59,
    "time": "2021-01-29T07:15:38.828Z",
    "type": "execution"
   },
   {
    "id": "ba3411b27d204a3691c81b7c45845b6b",
    "time": "2021-01-29T07:15:38.904Z",
    "type": "completion"
   },
   {
    "code": "acs5_covid_merge",
    "id": "8b807bf0461742fc8c7f3fee948db4b1",
    "idx": 60,
    "time": "2021-01-29T07:15:39.296Z",
    "type": "execution"
   },
   {
    "id": "8b807bf0461742fc8c7f3fee948db4b1",
    "time": "2021-01-29T07:15:39.383Z",
    "type": "completion"
   },
   {
    "code": "print(f\"There are {acs5_census.shape[0]} counties in the ACS 5-year Census dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")\nprint(f\"There are {acs5_covid_merge.shape[0]} counties in the merged prediction dataset with {fipsCoverage(acs5_census, all_fips)}% coverage.\")",
    "id": "795bb950059f4cdf943416604d401e28",
    "idx": 61,
    "time": "2021-01-29T07:15:39.945Z",
    "type": "execution"
   },
   {
    "id": "795bb950059f4cdf943416604d401e28",
    "time": "2021-01-29T07:15:40.023Z",
    "type": "completion"
   },
   {
    "code": "mapCounties(acs5_census, \ntitle= f\"Counties in acs5 dataset ({fipsCoverage(acs5_census, all_fips)}% coverage)\",\nsavefig= \"Counties_2019_ACS5.png\")",
    "id": "e0ce7ef58fa846169f951539d8d6175a",
    "idx": 62,
    "time": "2021-01-29T07:15:41.580Z",
    "type": "execution"
   },
   {
    "id": "e0ce7ef58fa846169f951539d8d6175a",
    "time": "2021-01-29T07:15:46.918Z",
    "type": "completion"
   },
   {
    "code": "def write_csv(df, path, filename):\n    \"\"\"\n    Takes a dataframe, path, and filename.\n    Returns a written csv.\n    Checks if the csv is already written, and if the path is correct.\n    \"\"\"\n    file = os.path.join(path, filename)\n\n    if os.path.exists(path):\n        if os.path.exists(file):\n            print(f\"The csv {filename} already exists.\")\n        else:\n            pd.DataFrame.to_csv(df, file)\n    else:\n        print(f\"The path {path} does not exist.\")",
    "id": "8dc477d7ffc14ea2840e60807cbab4f5",
    "idx": 64,
    "time": "2021-01-29T07:15:56.699Z",
    "type": "execution"
   },
   {
    "id": "8dc477d7ffc14ea2840e60807cbab4f5",
    "time": "2021-01-29T07:15:56.768Z",
    "type": "completion"
   },
   {
    "code": "census_weather_covid_inference_name = \"census_weather_covid_inference.csv\"\n\nwrite_csv(census_weather_covid_merge, clean_path, census_weather_covid_inference_name)",
    "id": "0862012f21e94d0fba14c5cf5a9ef8a0",
    "idx": 65,
    "time": "2021-01-29T07:15:57.985Z",
    "type": "execution"
   },
   {
    "id": "0862012f21e94d0fba14c5cf5a9ef8a0",
    "time": "2021-01-29T07:15:58.052Z",
    "type": "completion"
   },
   {
    "code": "census_covid_prediction_name = \"census_covid_prediction.csv\"\nwrite_csv(acs5_covid_merge, clean_path, census_covid_prediction_name)",
    "id": "6e42433979ca4354956e7d9b91b1fc26",
    "idx": 66,
    "time": "2021-01-29T07:16:11.694Z",
    "type": "execution"
   },
   {
    "id": "6e42433979ca4354956e7d9b91b1fc26",
    "time": "2021-01-29T07:16:11.816Z",
    "type": "completion"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
